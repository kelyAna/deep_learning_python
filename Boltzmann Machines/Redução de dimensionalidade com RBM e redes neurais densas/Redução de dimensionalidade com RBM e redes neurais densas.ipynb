{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redução de dimensionalidade com RBM e redes neurais densas\n",
    "\n",
    "**Objetivo:**\n",
    "O objetivo desta tarefa é comparar os resultados com e sem a utilização de RBM aplicado em uma rede neural densa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importações iniciais**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leitura da base de dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = datasets.load_digits()\n",
    "previsores = np.asarray(base.data, 'float32')\n",
    "classe = base.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalização dos dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizador = MinMaxScaler(feature_range=(0,1))\n",
    "previsores = normalizador.fit_transform(previsores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Divisão da base de dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criação e configuração da Restricted Boltzmann Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm = BernoulliRBM(random_state=0)\n",
    "rbm.n_iter = 25\n",
    "rbm.n_components = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_rbm = MLPClassifier(hidden_layer_sizes = (37, 37),\n",
    "                        activation = 'relu', \n",
    "                        solver = 'adam',\n",
    "                        batch_size = 50,\n",
    "                        max_iter = 1000,\n",
    "                        verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.27803311\n",
      "Iteration 2, loss = 2.02465515\n",
      "Iteration 3, loss = 1.69715020\n",
      "Iteration 4, loss = 1.31013431\n",
      "Iteration 5, loss = 0.96658360\n",
      "Iteration 6, loss = 0.71735117\n",
      "Iteration 7, loss = 0.56694610\n",
      "Iteration 8, loss = 0.47398984\n",
      "Iteration 9, loss = 0.41638579\n",
      "Iteration 10, loss = 0.37710257\n",
      "Iteration 11, loss = 0.34323846\n",
      "Iteration 12, loss = 0.32051375\n",
      "Iteration 13, loss = 0.30272654\n",
      "Iteration 14, loss = 0.28557530\n",
      "Iteration 15, loss = 0.27388845\n",
      "Iteration 16, loss = 0.26223601\n",
      "Iteration 17, loss = 0.25208354\n",
      "Iteration 18, loss = 0.24424159\n",
      "Iteration 19, loss = 0.23893515\n",
      "Iteration 20, loss = 0.23361197\n",
      "Iteration 21, loss = 0.22571198\n",
      "Iteration 22, loss = 0.22158597\n",
      "Iteration 23, loss = 0.22045840\n",
      "Iteration 24, loss = 0.21252001\n",
      "Iteration 25, loss = 0.20958937\n",
      "Iteration 26, loss = 0.20484689\n",
      "Iteration 27, loss = 0.20440410\n",
      "Iteration 28, loss = 0.19846539\n",
      "Iteration 29, loss = 0.19749115\n",
      "Iteration 30, loss = 0.19492831\n",
      "Iteration 31, loss = 0.18788084\n",
      "Iteration 32, loss = 0.18650538\n",
      "Iteration 33, loss = 0.18350852\n",
      "Iteration 34, loss = 0.18243158\n",
      "Iteration 35, loss = 0.18017230\n",
      "Iteration 36, loss = 0.17853468\n",
      "Iteration 37, loss = 0.17727773\n",
      "Iteration 38, loss = 0.17223925\n",
      "Iteration 39, loss = 0.17208388\n",
      "Iteration 40, loss = 0.16939686\n",
      "Iteration 41, loss = 0.16850975\n",
      "Iteration 42, loss = 0.16419725\n",
      "Iteration 43, loss = 0.16195295\n",
      "Iteration 44, loss = 0.16393562\n",
      "Iteration 45, loss = 0.16097809\n",
      "Iteration 46, loss = 0.15635382\n",
      "Iteration 47, loss = 0.15818150\n",
      "Iteration 48, loss = 0.15786982\n",
      "Iteration 49, loss = 0.15700269\n",
      "Iteration 50, loss = 0.15368115\n",
      "Iteration 51, loss = 0.14972746\n",
      "Iteration 52, loss = 0.15220741\n",
      "Iteration 53, loss = 0.14590479\n",
      "Iteration 54, loss = 0.14641306\n",
      "Iteration 55, loss = 0.14480299\n",
      "Iteration 56, loss = 0.14375572\n",
      "Iteration 57, loss = 0.14029521\n",
      "Iteration 58, loss = 0.14245110\n",
      "Iteration 59, loss = 0.14067829\n",
      "Iteration 60, loss = 0.13874890\n",
      "Iteration 61, loss = 0.13488968\n",
      "Iteration 62, loss = 0.13406267\n",
      "Iteration 63, loss = 0.13787670\n",
      "Iteration 64, loss = 0.13231748\n",
      "Iteration 65, loss = 0.13171714\n",
      "Iteration 66, loss = 0.13071917\n",
      "Iteration 67, loss = 0.12846497\n",
      "Iteration 68, loss = 0.12851933\n",
      "Iteration 69, loss = 0.12665012\n",
      "Iteration 70, loss = 0.12650951\n",
      "Iteration 71, loss = 0.12498007\n",
      "Iteration 72, loss = 0.12630815\n",
      "Iteration 73, loss = 0.12476572\n",
      "Iteration 74, loss = 0.12037031\n",
      "Iteration 75, loss = 0.12224102\n",
      "Iteration 76, loss = 0.12020949\n",
      "Iteration 77, loss = 0.11715312\n",
      "Iteration 78, loss = 0.11623807\n",
      "Iteration 79, loss = 0.11879971\n",
      "Iteration 80, loss = 0.11687114\n",
      "Iteration 81, loss = 0.11532888\n",
      "Iteration 82, loss = 0.11406119\n",
      "Iteration 83, loss = 0.11086901\n",
      "Iteration 84, loss = 0.11248530\n",
      "Iteration 85, loss = 0.11111030\n",
      "Iteration 86, loss = 0.11125007\n",
      "Iteration 87, loss = 0.11071352\n",
      "Iteration 88, loss = 0.11103620\n",
      "Iteration 89, loss = 0.10993043\n",
      "Iteration 90, loss = 0.10652092\n",
      "Iteration 91, loss = 0.10816363\n",
      "Iteration 92, loss = 0.10624013\n",
      "Iteration 93, loss = 0.10650195\n",
      "Iteration 94, loss = 0.10675127\n",
      "Iteration 95, loss = 0.10156792\n",
      "Iteration 96, loss = 0.10324533\n",
      "Iteration 97, loss = 0.10184797\n",
      "Iteration 98, loss = 0.10190966\n",
      "Iteration 99, loss = 0.10325437\n",
      "Iteration 100, loss = 0.10171698\n",
      "Iteration 101, loss = 0.10191605\n",
      "Iteration 102, loss = 0.09814723\n",
      "Iteration 103, loss = 0.09894320\n",
      "Iteration 104, loss = 0.09640797\n",
      "Iteration 105, loss = 0.09747939\n",
      "Iteration 106, loss = 0.09630673\n",
      "Iteration 107, loss = 0.09406484\n",
      "Iteration 108, loss = 0.09335282\n",
      "Iteration 109, loss = 0.09279481\n",
      "Iteration 110, loss = 0.09106307\n",
      "Iteration 111, loss = 0.09030485\n",
      "Iteration 112, loss = 0.09004132\n",
      "Iteration 113, loss = 0.09017925\n",
      "Iteration 114, loss = 0.08959588\n",
      "Iteration 115, loss = 0.08896269\n",
      "Iteration 116, loss = 0.08949336\n",
      "Iteration 117, loss = 0.08974913\n",
      "Iteration 118, loss = 0.08668865\n",
      "Iteration 119, loss = 0.08539055\n",
      "Iteration 120, loss = 0.08268410\n",
      "Iteration 121, loss = 0.08639923\n",
      "Iteration 122, loss = 0.08652403\n",
      "Iteration 123, loss = 0.08446829\n",
      "Iteration 124, loss = 0.08532470\n",
      "Iteration 125, loss = 0.08282359\n",
      "Iteration 126, loss = 0.08480316\n",
      "Iteration 127, loss = 0.08205424\n",
      "Iteration 128, loss = 0.08155154\n",
      "Iteration 129, loss = 0.08010789\n",
      "Iteration 130, loss = 0.07833495\n",
      "Iteration 131, loss = 0.07743047\n",
      "Iteration 132, loss = 0.07896041\n",
      "Iteration 133, loss = 0.07844165\n",
      "Iteration 134, loss = 0.07696856\n",
      "Iteration 135, loss = 0.07623604\n",
      "Iteration 136, loss = 0.07561620\n",
      "Iteration 137, loss = 0.07567046\n",
      "Iteration 138, loss = 0.07516629\n",
      "Iteration 139, loss = 0.07394417\n",
      "Iteration 140, loss = 0.07656267\n",
      "Iteration 141, loss = 0.07157332\n",
      "Iteration 142, loss = 0.07152104\n",
      "Iteration 143, loss = 0.07214204\n",
      "Iteration 144, loss = 0.07095483\n",
      "Iteration 145, loss = 0.06968498\n",
      "Iteration 146, loss = 0.07001919\n",
      "Iteration 147, loss = 0.07272106\n",
      "Iteration 148, loss = 0.07051295\n",
      "Iteration 149, loss = 0.07466398\n",
      "Iteration 150, loss = 0.06939045\n",
      "Iteration 151, loss = 0.06816156\n",
      "Iteration 152, loss = 0.06688705\n",
      "Iteration 153, loss = 0.06731314\n",
      "Iteration 154, loss = 0.06659054\n",
      "Iteration 155, loss = 0.06503910\n",
      "Iteration 156, loss = 0.06588037\n",
      "Iteration 157, loss = 0.06916586\n",
      "Iteration 158, loss = 0.06699409\n",
      "Iteration 159, loss = 0.06358640\n",
      "Iteration 160, loss = 0.06440508\n",
      "Iteration 161, loss = 0.06406046\n",
      "Iteration 162, loss = 0.06006421\n",
      "Iteration 163, loss = 0.06093955\n",
      "Iteration 164, loss = 0.06347549\n",
      "Iteration 165, loss = 0.06190666\n",
      "Iteration 166, loss = 0.06092276\n",
      "Iteration 167, loss = 0.05892329\n",
      "Iteration 168, loss = 0.05936761\n",
      "Iteration 169, loss = 0.05891904\n",
      "Iteration 170, loss = 0.05745950\n",
      "Iteration 171, loss = 0.05977452\n",
      "Iteration 172, loss = 0.05695069\n",
      "Iteration 173, loss = 0.05758523\n",
      "Iteration 174, loss = 0.05752154\n",
      "Iteration 175, loss = 0.05727916\n",
      "Iteration 176, loss = 0.05676378\n",
      "Iteration 177, loss = 0.05530460\n",
      "Iteration 178, loss = 0.05493542\n",
      "Iteration 179, loss = 0.05807927\n",
      "Iteration 180, loss = 0.05381004\n",
      "Iteration 181, loss = 0.05491925\n",
      "Iteration 182, loss = 0.05442703\n",
      "Iteration 183, loss = 0.05540121\n",
      "Iteration 184, loss = 0.05454350\n",
      "Iteration 185, loss = 0.05581225\n",
      "Iteration 186, loss = 0.05233550\n",
      "Iteration 187, loss = 0.05512952\n",
      "Iteration 188, loss = 0.05409045\n",
      "Iteration 189, loss = 0.05328210\n",
      "Iteration 190, loss = 0.05281143\n",
      "Iteration 191, loss = 0.05063565\n",
      "Iteration 192, loss = 0.05167032\n",
      "Iteration 193, loss = 0.05096485\n",
      "Iteration 194, loss = 0.04849413\n",
      "Iteration 195, loss = 0.04881306\n",
      "Iteration 196, loss = 0.04871653\n",
      "Iteration 197, loss = 0.04934106\n",
      "Iteration 198, loss = 0.05042943\n",
      "Iteration 199, loss = 0.04887052\n",
      "Iteration 200, loss = 0.04995800\n",
      "Iteration 201, loss = 0.04712971\n",
      "Iteration 202, loss = 0.04723922\n",
      "Iteration 203, loss = 0.04799450\n",
      "Iteration 204, loss = 0.04699524\n",
      "Iteration 205, loss = 0.04721801\n",
      "Iteration 206, loss = 0.04519030\n",
      "Iteration 207, loss = 0.04616362\n",
      "Iteration 208, loss = 0.04703491\n",
      "Iteration 209, loss = 0.04584035\n",
      "Iteration 210, loss = 0.04399347\n",
      "Iteration 211, loss = 0.04629257\n",
      "Iteration 212, loss = 0.04399596\n",
      "Iteration 213, loss = 0.04481021\n",
      "Iteration 214, loss = 0.04233361\n",
      "Iteration 215, loss = 0.04263520\n",
      "Iteration 216, loss = 0.04467135\n",
      "Iteration 217, loss = 0.04252230\n",
      "Iteration 218, loss = 0.04249400\n",
      "Iteration 219, loss = 0.04174687\n",
      "Iteration 220, loss = 0.04064712\n",
      "Iteration 221, loss = 0.04516910\n",
      "Iteration 222, loss = 0.04673309\n",
      "Iteration 223, loss = 0.04227530\n",
      "Iteration 224, loss = 0.04142334\n",
      "Iteration 225, loss = 0.04345778\n",
      "Iteration 226, loss = 0.04239451\n",
      "Iteration 227, loss = 0.04136361\n",
      "Iteration 228, loss = 0.04360522\n",
      "Iteration 229, loss = 0.04578379\n",
      "Iteration 230, loss = 0.03951251\n",
      "Iteration 231, loss = 0.03910148\n",
      "Iteration 232, loss = 0.03880094\n",
      "Iteration 233, loss = 0.03798538\n",
      "Iteration 234, loss = 0.03855326\n",
      "Iteration 235, loss = 0.03879308\n",
      "Iteration 236, loss = 0.03834873\n",
      "Iteration 237, loss = 0.03808700\n",
      "Iteration 238, loss = 0.03684196\n",
      "Iteration 239, loss = 0.03702204\n",
      "Iteration 240, loss = 0.03727927\n",
      "Iteration 241, loss = 0.03791137\n",
      "Iteration 242, loss = 0.03787183\n",
      "Iteration 243, loss = 0.03557785\n",
      "Iteration 244, loss = 0.03916258\n",
      "Iteration 245, loss = 0.03691066\n",
      "Iteration 246, loss = 0.03507549\n",
      "Iteration 247, loss = 0.03467816\n",
      "Iteration 248, loss = 0.03658604\n",
      "Iteration 249, loss = 0.03736067\n",
      "Iteration 250, loss = 0.03778016\n",
      "Iteration 251, loss = 0.03677894\n",
      "Iteration 252, loss = 0.03510320\n",
      "Iteration 253, loss = 0.03802233\n",
      "Iteration 254, loss = 0.03897209\n",
      "Iteration 255, loss = 0.03571892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 0.03429294\n",
      "Iteration 257, loss = 0.03505339\n",
      "Iteration 258, loss = 0.03377922\n",
      "Iteration 259, loss = 0.03268597\n",
      "Iteration 260, loss = 0.03279282\n",
      "Iteration 261, loss = 0.03487883\n",
      "Iteration 262, loss = 0.03439033\n",
      "Iteration 263, loss = 0.03492611\n",
      "Iteration 264, loss = 0.03239011\n",
      "Iteration 265, loss = 0.03388871\n",
      "Iteration 266, loss = 0.03207380\n",
      "Iteration 267, loss = 0.03191631\n",
      "Iteration 268, loss = 0.03295459\n",
      "Iteration 269, loss = 0.03332667\n",
      "Iteration 270, loss = 0.03233439\n",
      "Iteration 271, loss = 0.03132863\n",
      "Iteration 272, loss = 0.03309964\n",
      "Iteration 273, loss = 0.03082335\n",
      "Iteration 274, loss = 0.02995656\n",
      "Iteration 275, loss = 0.03023230\n",
      "Iteration 276, loss = 0.03205335\n",
      "Iteration 277, loss = 0.03228737\n",
      "Iteration 278, loss = 0.03109622\n",
      "Iteration 279, loss = 0.03346996\n",
      "Iteration 280, loss = 0.03184458\n",
      "Iteration 281, loss = 0.02969502\n",
      "Iteration 282, loss = 0.03064377\n",
      "Iteration 283, loss = 0.02923152\n",
      "Iteration 284, loss = 0.02940765\n",
      "Iteration 285, loss = 0.02949883\n",
      "Iteration 286, loss = 0.02961883\n",
      "Iteration 287, loss = 0.02909445\n",
      "Iteration 288, loss = 0.02919837\n",
      "Iteration 289, loss = 0.02812880\n",
      "Iteration 290, loss = 0.02776195\n",
      "Iteration 291, loss = 0.02893166\n",
      "Iteration 292, loss = 0.02742482\n",
      "Iteration 293, loss = 0.02822426\n",
      "Iteration 294, loss = 0.02742426\n",
      "Iteration 295, loss = 0.02679262\n",
      "Iteration 296, loss = 0.02898412\n",
      "Iteration 297, loss = 0.02807965\n",
      "Iteration 298, loss = 0.02829522\n",
      "Iteration 299, loss = 0.02835799\n",
      "Iteration 300, loss = 0.02676209\n",
      "Iteration 301, loss = 0.02623666\n",
      "Iteration 302, loss = 0.02798701\n",
      "Iteration 303, loss = 0.02664186\n",
      "Iteration 304, loss = 0.02562106\n",
      "Iteration 305, loss = 0.02773628\n",
      "Iteration 306, loss = 0.02750074\n",
      "Iteration 307, loss = 0.02546876\n",
      "Iteration 308, loss = 0.02789060\n",
      "Iteration 309, loss = 0.02584796\n",
      "Iteration 310, loss = 0.02669266\n",
      "Iteration 311, loss = 0.02588210\n",
      "Iteration 312, loss = 0.02497576\n",
      "Iteration 313, loss = 0.02446316\n",
      "Iteration 314, loss = 0.02587166\n",
      "Iteration 315, loss = 0.02437376\n",
      "Iteration 316, loss = 0.02624390\n",
      "Iteration 317, loss = 0.02337846\n",
      "Iteration 318, loss = 0.02544247\n",
      "Iteration 319, loss = 0.02489104\n",
      "Iteration 320, loss = 0.02608619\n",
      "Iteration 321, loss = 0.02732302\n",
      "Iteration 322, loss = 0.02449069\n",
      "Iteration 323, loss = 0.02489987\n",
      "Iteration 324, loss = 0.02460876\n",
      "Iteration 325, loss = 0.02289645\n",
      "Iteration 326, loss = 0.02449985\n",
      "Iteration 327, loss = 0.02346965\n",
      "Iteration 328, loss = 0.02432777\n",
      "Iteration 329, loss = 0.02403175\n",
      "Iteration 330, loss = 0.02419322\n",
      "Iteration 331, loss = 0.02550227\n",
      "Iteration 332, loss = 0.02758672\n",
      "Iteration 333, loss = 0.02707331\n",
      "Iteration 334, loss = 0.02388544\n",
      "Iteration 335, loss = 0.02380905\n",
      "Iteration 336, loss = 0.02440771\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('rbm', BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=50, n_iter=25,\n",
       "       random_state=0, verbose=0)), ('naive', MLPClassifier(activation='relu', alpha=0.0001, batch_size=50, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(37, 3...ffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=False))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificador_rbm = Pipeline(steps = [('rbm', rbm), ('naive', mlp_rbm)])\n",
    "classificador_rbm.fit(previsores_treinamento, classe_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes_rbm = classificador_rbm.predict(previsores_teste)\n",
    "precisao_rbm = metrics.accuracy_score(previsoes_rbm, classe_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.20145694\n",
      "Iteration 2, loss = 1.86924465\n",
      "Iteration 3, loss = 1.47539235\n",
      "Iteration 4, loss = 1.06622176\n",
      "Iteration 5, loss = 0.76062855\n",
      "Iteration 6, loss = 0.56221122\n",
      "Iteration 7, loss = 0.44249061\n",
      "Iteration 8, loss = 0.36013939\n",
      "Iteration 9, loss = 0.30240949\n",
      "Iteration 10, loss = 0.26868798\n",
      "Iteration 11, loss = 0.23462271\n",
      "Iteration 12, loss = 0.21094471\n",
      "Iteration 13, loss = 0.18915155\n",
      "Iteration 14, loss = 0.17427676\n",
      "Iteration 15, loss = 0.16174995\n",
      "Iteration 16, loss = 0.15058830\n",
      "Iteration 17, loss = 0.13973216\n",
      "Iteration 18, loss = 0.13066519\n",
      "Iteration 19, loss = 0.12384343\n",
      "Iteration 20, loss = 0.11455146\n",
      "Iteration 21, loss = 0.10922760\n",
      "Iteration 22, loss = 0.10423101\n",
      "Iteration 23, loss = 0.09916214\n",
      "Iteration 24, loss = 0.09396014\n",
      "Iteration 25, loss = 0.08807617\n",
      "Iteration 26, loss = 0.08334574\n",
      "Iteration 27, loss = 0.08178102\n",
      "Iteration 28, loss = 0.07839646\n",
      "Iteration 29, loss = 0.07308869\n",
      "Iteration 30, loss = 0.06942575\n",
      "Iteration 31, loss = 0.06512404\n",
      "Iteration 32, loss = 0.06506424\n",
      "Iteration 33, loss = 0.06377576\n",
      "Iteration 34, loss = 0.05932657\n",
      "Iteration 35, loss = 0.05634647\n",
      "Iteration 36, loss = 0.05428717\n",
      "Iteration 37, loss = 0.05115434\n",
      "Iteration 38, loss = 0.04985195\n",
      "Iteration 39, loss = 0.04681076\n",
      "Iteration 40, loss = 0.04539356\n",
      "Iteration 41, loss = 0.04438240\n",
      "Iteration 42, loss = 0.04305778\n",
      "Iteration 43, loss = 0.04046911\n",
      "Iteration 44, loss = 0.03890777\n",
      "Iteration 45, loss = 0.03734107\n",
      "Iteration 46, loss = 0.03665948\n",
      "Iteration 47, loss = 0.03494105\n",
      "Iteration 48, loss = 0.03408446\n",
      "Iteration 49, loss = 0.03483107\n",
      "Iteration 50, loss = 0.03150253\n",
      "Iteration 51, loss = 0.03121085\n",
      "Iteration 52, loss = 0.03007937\n",
      "Iteration 53, loss = 0.02818179\n",
      "Iteration 54, loss = 0.02714952\n",
      "Iteration 55, loss = 0.02669179\n",
      "Iteration 56, loss = 0.02604498\n",
      "Iteration 57, loss = 0.02501968\n",
      "Iteration 58, loss = 0.02385310\n",
      "Iteration 59, loss = 0.02288895\n",
      "Iteration 60, loss = 0.02281768\n",
      "Iteration 61, loss = 0.02127229\n",
      "Iteration 62, loss = 0.02229101\n",
      "Iteration 63, loss = 0.01999684\n",
      "Iteration 64, loss = 0.02068912\n",
      "Iteration 65, loss = 0.01954229\n",
      "Iteration 66, loss = 0.01914154\n",
      "Iteration 67, loss = 0.01792132\n",
      "Iteration 68, loss = 0.01756328\n",
      "Iteration 69, loss = 0.01673005\n",
      "Iteration 70, loss = 0.01642924\n",
      "Iteration 71, loss = 0.01629860\n",
      "Iteration 72, loss = 0.01555387\n",
      "Iteration 73, loss = 0.01458330\n",
      "Iteration 74, loss = 0.01451384\n",
      "Iteration 75, loss = 0.01414499\n",
      "Iteration 76, loss = 0.01388434\n",
      "Iteration 77, loss = 0.01327600\n",
      "Iteration 78, loss = 0.01291137\n",
      "Iteration 79, loss = 0.01194624\n",
      "Iteration 80, loss = 0.01204632\n",
      "Iteration 81, loss = 0.01156975\n",
      "Iteration 82, loss = 0.01100863\n",
      "Iteration 83, loss = 0.01090104\n",
      "Iteration 84, loss = 0.01043060\n",
      "Iteration 85, loss = 0.01031755\n",
      "Iteration 86, loss = 0.00987219\n",
      "Iteration 87, loss = 0.00975648\n",
      "Iteration 88, loss = 0.00951763\n",
      "Iteration 89, loss = 0.00913984\n",
      "Iteration 90, loss = 0.00890148\n",
      "Iteration 91, loss = 0.00861865\n",
      "Iteration 92, loss = 0.00831267\n",
      "Iteration 93, loss = 0.00820557\n",
      "Iteration 94, loss = 0.00830167\n",
      "Iteration 95, loss = 0.00740892\n",
      "Iteration 96, loss = 0.00747578\n",
      "Iteration 97, loss = 0.00745056\n",
      "Iteration 98, loss = 0.00728731\n",
      "Iteration 99, loss = 0.00702788\n",
      "Iteration 100, loss = 0.00709001\n",
      "Iteration 101, loss = 0.00660082\n",
      "Iteration 102, loss = 0.00646996\n",
      "Iteration 103, loss = 0.00614757\n",
      "Iteration 104, loss = 0.00657839\n",
      "Iteration 105, loss = 0.00625863\n",
      "Iteration 106, loss = 0.00619650\n",
      "Iteration 107, loss = 0.00549995\n",
      "Iteration 108, loss = 0.00556382\n",
      "Iteration 109, loss = 0.00539705\n",
      "Iteration 110, loss = 0.00499317\n",
      "Iteration 111, loss = 0.00498063\n",
      "Iteration 112, loss = 0.00489309\n",
      "Iteration 113, loss = 0.00482358\n",
      "Iteration 114, loss = 0.00467572\n",
      "Iteration 115, loss = 0.00456405\n",
      "Iteration 116, loss = 0.00446181\n",
      "Iteration 117, loss = 0.00434391\n",
      "Iteration 118, loss = 0.00420282\n",
      "Iteration 119, loss = 0.00444506\n",
      "Iteration 120, loss = 0.00388037\n",
      "Iteration 121, loss = 0.00379989\n",
      "Iteration 122, loss = 0.00382096\n",
      "Iteration 123, loss = 0.00366777\n",
      "Iteration 124, loss = 0.00365832\n",
      "Iteration 125, loss = 0.00344563\n",
      "Iteration 126, loss = 0.00360075\n",
      "Iteration 127, loss = 0.00345723\n",
      "Iteration 128, loss = 0.00330536\n",
      "Iteration 129, loss = 0.00322789\n",
      "Iteration 130, loss = 0.00324748\n",
      "Iteration 131, loss = 0.00307487\n",
      "Iteration 132, loss = 0.00298092\n",
      "Iteration 133, loss = 0.00286167\n",
      "Iteration 134, loss = 0.00280039\n",
      "Iteration 135, loss = 0.00275496\n",
      "Iteration 136, loss = 0.00274242\n",
      "Iteration 137, loss = 0.00270962\n",
      "Iteration 138, loss = 0.00266290\n",
      "Iteration 139, loss = 0.00258478\n",
      "Iteration 140, loss = 0.00252818\n",
      "Iteration 141, loss = 0.00231250\n",
      "Iteration 142, loss = 0.00236994\n",
      "Iteration 143, loss = 0.00237168\n",
      "Iteration 144, loss = 0.00252550\n",
      "Iteration 145, loss = 0.00234468\n",
      "Iteration 146, loss = 0.00214551\n",
      "Iteration 147, loss = 0.00210151\n",
      "Iteration 148, loss = 0.00212524\n",
      "Iteration 149, loss = 0.00204560\n",
      "Iteration 150, loss = 0.00206867\n",
      "Iteration 151, loss = 0.00196366\n",
      "Iteration 152, loss = 0.00195693\n",
      "Iteration 153, loss = 0.00190479\n",
      "Iteration 154, loss = 0.00188853\n",
      "Iteration 155, loss = 0.00182078\n",
      "Iteration 156, loss = 0.00187592\n",
      "Iteration 157, loss = 0.00175065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlp_simples = MLPClassifier(hidden_layer_sizes = (37, 37),\n",
    "                        activation = 'relu', \n",
    "                        solver = 'adam',\n",
    "                        batch_size = 50,\n",
    "                        max_iter = 1000,\n",
    "                        verbose = 1)\n",
    "mlp_simples.fit(previsores_treinamento, classe_treinamento)\n",
    "previsoes_mlp = mlp_simples.predict(previsores_teste)\n",
    "precisao_mlp = metrics.accuracy_score(previsoes_mlp, classe_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
