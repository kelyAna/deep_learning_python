{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uso de uma Rede Neural Recorrente, para prever investimentos na bolsa de valores**\n",
    "\n",
    "A base de dados representa investimentos nas ações da Petrobas na bolsa de valores, com o histórico do ano de 2013 ao ano de 2018, em tempos de crise. Dessa forma, o objetivo principal do modelo é gerar um gráfico dos preços das ações, mesmo em tempos de crise.\n",
    "\n",
    "Você pode encontrar a base de dados, em:\n",
    "\n",
    "https://br.financas.yahoo.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importações iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando da base de dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv('petr4_treinamento.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A base de treino é composta pelas seguintes features:\n",
    "\n",
    "* Date - Referente à data de abertura da ação;\n",
    "\n",
    "* Open - Referente ao valor na abertura da ação;\n",
    "\n",
    "* High - Referente ao valor máximo da ação;\n",
    "\n",
    "* Low - Referente ao valor mínimo da ação;\n",
    "\n",
    "* Close - Referente ao valor que a ação foi fechada;\n",
    "\n",
    "* Adj Close - Referente ao valor real previsto pela ação;\n",
    "\n",
    "* Volume - Total de investimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>19.639999</td>\n",
       "      <td>19.870001</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>18.077084</td>\n",
       "      <td>24361100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2013-01-10</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.540001</td>\n",
       "      <td>19.830000</td>\n",
       "      <td>18.214869</td>\n",
       "      <td>17526200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2013-01-11</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>19.879999</td>\n",
       "      <td>18.260794</td>\n",
       "      <td>18223600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2013-01-14</td>\n",
       "      <td>20.010000</td>\n",
       "      <td>20.240000</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.719999</td>\n",
       "      <td>18.113827</td>\n",
       "      <td>28302400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2013-01-15</td>\n",
       "      <td>20.010000</td>\n",
       "      <td>20.240000</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.820000</td>\n",
       "      <td>18.205681</td>\n",
       "      <td>29633900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2013-01-16</td>\n",
       "      <td>19.889999</td>\n",
       "      <td>19.889999</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>19.840000</td>\n",
       "      <td>18.224054</td>\n",
       "      <td>16787800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2013-01-17</td>\n",
       "      <td>19.860001</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>19.709999</td>\n",
       "      <td>18.104639</td>\n",
       "      <td>19719600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>19.799999</td>\n",
       "      <td>19.889999</td>\n",
       "      <td>19.540001</td>\n",
       "      <td>19.580000</td>\n",
       "      <td>17.985229</td>\n",
       "      <td>18913900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013-01-21</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>19.270000</td>\n",
       "      <td>19.389999</td>\n",
       "      <td>17.810705</td>\n",
       "      <td>18086200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2013-01-22</td>\n",
       "      <td>19.420000</td>\n",
       "      <td>19.610001</td>\n",
       "      <td>19.230000</td>\n",
       "      <td>19.580000</td>\n",
       "      <td>17.985229</td>\n",
       "      <td>23535100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2013-01-23</td>\n",
       "      <td>19.420000</td>\n",
       "      <td>19.629999</td>\n",
       "      <td>19.230000</td>\n",
       "      <td>19.549999</td>\n",
       "      <td>17.957674</td>\n",
       "      <td>17200800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2013-01-24</td>\n",
       "      <td>19.370001</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>19.370001</td>\n",
       "      <td>19.610001</td>\n",
       "      <td>18.012787</td>\n",
       "      <td>19612600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2013-01-28</td>\n",
       "      <td>19.730000</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>19.270000</td>\n",
       "      <td>19.370001</td>\n",
       "      <td>17.792336</td>\n",
       "      <td>20122600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2013-01-29</td>\n",
       "      <td>19.350000</td>\n",
       "      <td>19.370001</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>19.110001</td>\n",
       "      <td>17.553513</td>\n",
       "      <td>27097900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2013-01-30</td>\n",
       "      <td>18.990000</td>\n",
       "      <td>18.990000</td>\n",
       "      <td>18.170000</td>\n",
       "      <td>18.200001</td>\n",
       "      <td>16.717628</td>\n",
       "      <td>66985800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2013-01-31</td>\n",
       "      <td>18.260000</td>\n",
       "      <td>18.330000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>18.080000</td>\n",
       "      <td>16.607405</td>\n",
       "      <td>33246400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>18.139999</td>\n",
       "      <td>18.650000</td>\n",
       "      <td>18.120001</td>\n",
       "      <td>18.459999</td>\n",
       "      <td>16.956451</td>\n",
       "      <td>28860400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2013-02-04</td>\n",
       "      <td>18.160000</td>\n",
       "      <td>18.350000</td>\n",
       "      <td>17.889999</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>16.533920</td>\n",
       "      <td>32294300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>17.280001</td>\n",
       "      <td>18.290001</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>18.080000</td>\n",
       "      <td>16.607405</td>\n",
       "      <td>77332900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2013-02-06</td>\n",
       "      <td>17.930000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.530001</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>16.166500</td>\n",
       "      <td>30886100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2013-02-07</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>17.980000</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>16.074644</td>\n",
       "      <td>37066400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2013-02-08</td>\n",
       "      <td>17.549999</td>\n",
       "      <td>17.780001</td>\n",
       "      <td>17.379999</td>\n",
       "      <td>17.740000</td>\n",
       "      <td>16.295095</td>\n",
       "      <td>31068100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2013-02-13</td>\n",
       "      <td>17.950001</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.730000</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>16.350208</td>\n",
       "      <td>22328200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2013-02-14</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>17.580000</td>\n",
       "      <td>17.670000</td>\n",
       "      <td>16.230797</td>\n",
       "      <td>25902900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2013-02-15</td>\n",
       "      <td>17.610001</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>17.530001</td>\n",
       "      <td>17.629999</td>\n",
       "      <td>16.194056</td>\n",
       "      <td>25001600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>2017-11-20</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>15.988025</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>16.150000</td>\n",
       "      <td>16.309999</td>\n",
       "      <td>15.850000</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>15.868263</td>\n",
       "      <td>45817800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>2017-11-22</td>\n",
       "      <td>16.090000</td>\n",
       "      <td>16.240000</td>\n",
       "      <td>15.930000</td>\n",
       "      <td>16.110001</td>\n",
       "      <td>16.077845</td>\n",
       "      <td>37444900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>16.260000</td>\n",
       "      <td>15.940000</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.157686</td>\n",
       "      <td>15403600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>2017-11-24</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>16.370001</td>\n",
       "      <td>16.040001</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>18790700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>16.010000</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>15.870000</td>\n",
       "      <td>15.838324</td>\n",
       "      <td>28445800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>15.930000</td>\n",
       "      <td>16.040001</td>\n",
       "      <td>15.810000</td>\n",
       "      <td>15.840000</td>\n",
       "      <td>15.808384</td>\n",
       "      <td>30429600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>15.870000</td>\n",
       "      <td>15.920000</td>\n",
       "      <td>15.320000</td>\n",
       "      <td>15.330000</td>\n",
       "      <td>15.299401</td>\n",
       "      <td>45973000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>15.470000</td>\n",
       "      <td>14.990000</td>\n",
       "      <td>15.380000</td>\n",
       "      <td>15.349302</td>\n",
       "      <td>52811400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>15.340000</td>\n",
       "      <td>15.770000</td>\n",
       "      <td>15.260000</td>\n",
       "      <td>15.610000</td>\n",
       "      <td>15.578842</td>\n",
       "      <td>42703800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>15.650000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>15.460000</td>\n",
       "      <td>15.480000</td>\n",
       "      <td>15.449101</td>\n",
       "      <td>43821500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>15.830000</td>\n",
       "      <td>15.210000</td>\n",
       "      <td>15.310000</td>\n",
       "      <td>15.279442</td>\n",
       "      <td>30228000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>2017-12-06</td>\n",
       "      <td>15.220000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>15.520000</td>\n",
       "      <td>15.489023</td>\n",
       "      <td>39238500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>2017-12-07</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>15.490000</td>\n",
       "      <td>15.070000</td>\n",
       "      <td>15.260000</td>\n",
       "      <td>15.229542</td>\n",
       "      <td>37281400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>15.510000</td>\n",
       "      <td>15.680000</td>\n",
       "      <td>15.350000</td>\n",
       "      <td>15.350000</td>\n",
       "      <td>15.319362</td>\n",
       "      <td>39584500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>15.480000</td>\n",
       "      <td>15.570000</td>\n",
       "      <td>15.370000</td>\n",
       "      <td>15.380000</td>\n",
       "      <td>15.349302</td>\n",
       "      <td>21281600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>2017-12-12</td>\n",
       "      <td>15.360000</td>\n",
       "      <td>15.490000</td>\n",
       "      <td>15.180000</td>\n",
       "      <td>15.490000</td>\n",
       "      <td>15.459082</td>\n",
       "      <td>36201200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>15.650000</td>\n",
       "      <td>15.680000</td>\n",
       "      <td>15.110000</td>\n",
       "      <td>15.180000</td>\n",
       "      <td>15.149701</td>\n",
       "      <td>46828900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>2017-12-14</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>15.310000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.010000</td>\n",
       "      <td>14.980041</td>\n",
       "      <td>37177300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>15.050000</td>\n",
       "      <td>15.240000</td>\n",
       "      <td>14.950000</td>\n",
       "      <td>14.950000</td>\n",
       "      <td>14.920160</td>\n",
       "      <td>55668300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>2017-12-18</td>\n",
       "      <td>15.160000</td>\n",
       "      <td>15.330000</td>\n",
       "      <td>15.130000</td>\n",
       "      <td>15.220000</td>\n",
       "      <td>15.189621</td>\n",
       "      <td>42760400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>2017-12-19</td>\n",
       "      <td>15.180000</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>15.060000</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>15.109781</td>\n",
       "      <td>22639700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>2017-12-20</td>\n",
       "      <td>15.210000</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>15.170000</td>\n",
       "      <td>15.240000</td>\n",
       "      <td>15.209581</td>\n",
       "      <td>20149700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>2017-12-21</td>\n",
       "      <td>15.310000</td>\n",
       "      <td>15.870000</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>15.860000</td>\n",
       "      <td>15.828343</td>\n",
       "      <td>47219400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>2017-12-22</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.890000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.718563</td>\n",
       "      <td>18708500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.718563</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>15.970000</td>\n",
       "      <td>15.938125</td>\n",
       "      <td>22173100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>16.139999</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>16.049999</td>\n",
       "      <td>16.017963</td>\n",
       "      <td>23552200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.129999</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>19011500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1245 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "0     2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1     2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2     2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3     2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4     2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "5     2013-01-09  19.639999  19.870001  19.459999  19.680000  18.077084   \n",
       "6     2013-01-10  19.770000  20.049999  19.540001  19.830000  18.214869   \n",
       "7     2013-01-11  19.850000  20.040001  19.700001  19.879999  18.260794   \n",
       "8     2013-01-14  20.010000  20.240000  19.690001  19.719999  18.113827   \n",
       "9     2013-01-15  20.010000  20.240000  19.690001  19.820000  18.205681   \n",
       "10    2013-01-16  19.889999  19.889999  19.600000  19.840000  18.224054   \n",
       "11    2013-01-17  19.860001  19.930000  19.600000  19.709999  18.104639   \n",
       "12    2013-01-18  19.799999  19.889999  19.540001  19.580000  17.985229   \n",
       "13    2013-01-21  19.570000  19.600000  19.270000  19.389999  17.810705   \n",
       "14    2013-01-22  19.420000  19.610001  19.230000  19.580000  17.985229   \n",
       "15    2013-01-23  19.420000  19.629999  19.230000  19.549999  17.957674   \n",
       "16    2013-01-24  19.370001  19.750000  19.370001  19.610001  18.012787   \n",
       "17    2013-01-28  19.730000  19.809999  19.270000  19.370001  17.792336   \n",
       "18    2013-01-29  19.350000  19.370001  18.840000  19.110001  17.553513   \n",
       "19    2013-01-30  18.990000  18.990000  18.170000  18.200001  16.717628   \n",
       "20    2013-01-31  18.260000  18.330000  17.900000  18.080000  16.607405   \n",
       "21    2013-02-01  18.139999  18.650000  18.120001  18.459999  16.956451   \n",
       "22    2013-02-04  18.160000  18.350000  17.889999  18.000000  16.533920   \n",
       "23    2013-02-05  17.280001  18.290001  17.260000  18.080000  16.607405   \n",
       "24    2013-02-06  17.930000  18.000000  17.530001  17.600000  16.166500   \n",
       "25    2013-02-07  17.750000  17.980000  17.320000  17.500000  16.074644   \n",
       "26    2013-02-08  17.549999  17.780001  17.379999  17.740000  16.295095   \n",
       "27    2013-02-13  17.950001  18.000000  17.730000  17.799999  16.350208   \n",
       "28    2013-02-14  17.790001  17.900000  17.580000  17.670000  16.230797   \n",
       "29    2013-02-15  17.610001  17.790001  17.530001  17.629999  16.194056   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "1215  2017-11-20  16.020000  16.020000  16.020000  16.020000  15.988025   \n",
       "1216  2017-11-21  16.150000  16.309999  15.850000  15.900000  15.868263   \n",
       "1217  2017-11-22  16.090000  16.240000  15.930000  16.110001  16.077845   \n",
       "1218  2017-11-23  15.980000  16.260000  15.940000  16.190001  16.157686   \n",
       "1219  2017-11-24  16.250000  16.370001  16.040001  16.100000  16.067865   \n",
       "1220  2017-11-27  16.010000  16.020000  15.780000  15.870000  15.838324   \n",
       "1221  2017-11-28  15.930000  16.040001  15.810000  15.840000  15.808384   \n",
       "1222  2017-11-29  15.870000  15.920000  15.320000  15.330000  15.299401   \n",
       "1223  2017-11-30  15.300000  15.470000  14.990000  15.380000  15.349302   \n",
       "1224  2017-12-01  15.340000  15.770000  15.260000  15.610000  15.578842   \n",
       "1225  2017-12-04  15.650000  15.800000  15.460000  15.480000  15.449101   \n",
       "1226  2017-12-05  15.500000  15.830000  15.210000  15.310000  15.279442   \n",
       "1227  2017-12-06  15.220000  15.700000  15.140000  15.520000  15.489023   \n",
       "1228  2017-12-07  15.300000  15.490000  15.070000  15.260000  15.229542   \n",
       "1229  2017-12-08  15.510000  15.680000  15.350000  15.350000  15.319362   \n",
       "1230  2017-12-11  15.480000  15.570000  15.370000  15.380000  15.349302   \n",
       "1231  2017-12-12  15.360000  15.490000  15.180000  15.490000  15.459082   \n",
       "1232  2017-12-13  15.650000  15.680000  15.110000  15.180000  15.149701   \n",
       "1233  2017-12-14  15.100000  15.310000  15.000000  15.010000  14.980041   \n",
       "1234  2017-12-15  15.050000  15.240000  14.950000  14.950000  14.920160   \n",
       "1235  2017-12-18  15.160000  15.330000  15.130000  15.220000  15.189621   \n",
       "1236  2017-12-19  15.180000  15.250000  15.060000  15.140000  15.109781   \n",
       "1237  2017-12-20  15.210000  15.300000  15.170000  15.240000  15.209581   \n",
       "1238  2017-12-21  15.310000  15.870000  15.300000  15.860000  15.828343   \n",
       "1239  2017-12-22  15.750000  15.890000  15.690000  15.750000  15.718563   \n",
       "1240  2017-12-25  15.750000  15.750000  15.750000  15.750000  15.718563   \n",
       "1241  2017-12-26  15.750000  15.990000  15.690000  15.970000  15.938125   \n",
       "1242  2017-12-27  15.990000  16.139999  15.980000  16.049999  16.017963   \n",
       "1243  2017-12-28  16.100000  16.129999  16.000000  16.100000  16.067865   \n",
       "1244  2017-12-29  16.100000  16.100000  16.100000  16.100000  16.067865   \n",
       "\n",
       "          Volume  \n",
       "0     30182600.0  \n",
       "1     30552600.0  \n",
       "2     36141000.0  \n",
       "3     28069600.0  \n",
       "4     29091300.0  \n",
       "5     24361100.0  \n",
       "6     17526200.0  \n",
       "7     18223600.0  \n",
       "8     28302400.0  \n",
       "9     29633900.0  \n",
       "10    16787800.0  \n",
       "11    19719600.0  \n",
       "12    18913900.0  \n",
       "13    18086200.0  \n",
       "14    23535100.0  \n",
       "15    17200800.0  \n",
       "16    19612600.0  \n",
       "17    20122600.0  \n",
       "18    27097900.0  \n",
       "19    66985800.0  \n",
       "20    33246400.0  \n",
       "21    28860400.0  \n",
       "22    32294300.0  \n",
       "23    77332900.0  \n",
       "24    30886100.0  \n",
       "25    37066400.0  \n",
       "26    31068100.0  \n",
       "27    22328200.0  \n",
       "28    25902900.0  \n",
       "29    25001600.0  \n",
       "...          ...  \n",
       "1215         0.0  \n",
       "1216  45817800.0  \n",
       "1217  37444900.0  \n",
       "1218  15403600.0  \n",
       "1219  18790700.0  \n",
       "1220  28445800.0  \n",
       "1221  30429600.0  \n",
       "1222  45973000.0  \n",
       "1223  52811400.0  \n",
       "1224  42703800.0  \n",
       "1225  43821500.0  \n",
       "1226  30228000.0  \n",
       "1227  39238500.0  \n",
       "1228  37281400.0  \n",
       "1229  39584500.0  \n",
       "1230  21281600.0  \n",
       "1231  36201200.0  \n",
       "1232  46828900.0  \n",
       "1233  37177300.0  \n",
       "1234  55668300.0  \n",
       "1235  42760400.0  \n",
       "1236  22639700.0  \n",
       "1237  20149700.0  \n",
       "1238  47219400.0  \n",
       "1239  18708500.0  \n",
       "1240         0.0  \n",
       "1241  22173100.0  \n",
       "1242  23552200.0  \n",
       "1243  19011500.0  \n",
       "1244         0.0  \n",
       "\n",
       "[1245 rows x 7 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pré-processamento dos dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em relação ao valores NaN, podemos realizar três ações:\n",
    "\n",
    "* Exclução desses valores\n",
    "* Substituição por 0\n",
    "* Substituir pela média de cada coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         3\n",
       "High         3\n",
       "Low          3\n",
       "Close        3\n",
       "Adj Close    3\n",
       "Volume       3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visto que a quantidade de valores do tipo NaN é relativamente baixa em relação aos 1245 dados do data set, podemos optar por apagar esses registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = dataFrame.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1242, 7)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, podemos verificar se a função dropna(), conseguiu apagar estes valores nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse cenário, faremos a previsão dos valores das ações com a utilização de múltiplos previsores. Com isso, usaremos todos os atributos da base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treinamento = dataFrame.iloc[:,1:7].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1242, 6)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_treinamento.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso modelo trabalha com valores reais, podemos dizer assim. Dessa forma, podemos encontrar valores muito altos, que podem influenciar no treinamento. Para resolver essa problemática, podemos escolher duas opções:\n",
    "\n",
    "* Normalização dos dados\n",
    "\n",
    "* Padronização dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambas as técnicas trabalham com o um único objetivo: deixar os dados na mesma grandeza. Para isso, veremos como cada uma trabalha, de forma individual:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalização**\n",
    "\n",
    "![Min-Max fórmula](https://miro.medium.com/max/202/1*9N7QdpE_CfvkTyirk7_oWw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padronização**\n",
    "\n",
    "<img src=\"https://d1whtlypfis84e.cloudfront.net/guides/wp-content/uploads/2020/04/04155631/1426878678.png \" alt=\"drawing\" width=\"150\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste modelo, vamos utilizar a técnica de **Normalização**, visto a tratar os outliers nos nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para normalizar esse dados, vamos utilizar a função MinMaxScaler(), que irá transformar os dados em uma escala de 0 e 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizador = MinMaxScaler(feature_range = [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treinamento_normalizer = normalizador.fit_transform(base_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938],\n",
       "       [0.7562984 ],\n",
       "       [0.78149225],\n",
       "       ...,\n",
       "       [0.57122093],\n",
       "       [0.57655039],\n",
       "       [0.57655039]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizador_previsao = MinMaxScaler(feature_range = [0,1])\n",
    "\n",
    "normalizador_previsao.fit_transform(base_treinamento[:, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938, 0.77266112, 0.79682707, 0.76080559, 0.6838135 ,\n",
       "        0.04318274],\n",
       "       [0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "        0.0437121 ],\n",
       "       [0.78149225, 0.79253519, 0.82139202, 0.79715132, 0.71726583,\n",
       "        0.05170752],\n",
       "       ...,\n",
       "       [0.57122093, 0.57537562, 0.60696008, 0.58202356, 0.58202349,\n",
       "        0.03369652],\n",
       "       [0.57655039, 0.57489089, 0.60798362, 0.5844794 , 0.58447937,\n",
       "        0.02720006],\n",
       "       [0.57655039, 0.57343674, 0.61310133, 0.5844794 , 0.58447937,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_treinamento_normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estrutura da base para previsão temporal I**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando trabalha-se com dados temporais é preciso definir um intervalo de tempo entre os dados e o alvo. Neste caso, podemos pegar um registro que tenha registros anteriores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo: \n",
    "\n",
    "| Dia da semana | Dia | Preço |\n",
    "|---------------|-----|-------|\n",
    "| Quinta-feira  | 03  | 19,99 |\n",
    "| Sexta-feira   | 04  | 19,80 |\n",
    "| Segunda-feira | 07  | 20,33 |\n",
    "| Terça-feira   | 08  | 20,48 |\n",
    "| Quarta-feira  | 09  | 20,11 |\n",
    "\n",
    "Para prever o preço da Quarta-feira, dia 09, precisamos pegar os valores das 4 datas anteriores:\n",
    "\n",
    "| **Previsores** |   **Preço real**|\n",
    "|---------------|-----|\n",
    "| 19,99 19,80 20,33 20,48 20,11  | 20,11  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estrutura da base para previsão temporal II**\n",
    "\n",
    "* Prever o preço real através do 90 valores anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = []\n",
    "preco_real = []\n",
    "\n",
    "for i in range(90, 1242):\n",
    "    previsores.append(base_treinamento_normalizer[i-90:i, 0:6])\n",
    "    preco_real.append(base_treinamento_normalizer[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores, preco_real = np.array(previsores), np.array(preco_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 90, 6)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a utilização da biblioteca Keras, nosso vetor de entradas precisa manter uma dimensão 3D com o formato:\n",
    "\n",
    "                (batch_size, timesteps, input_dim)\n",
    "Sendo:\n",
    "\n",
    "* Batch_size - refere-se ao número de exemplos de treinamento usados em uma intereção\n",
    "\n",
    "* Timesteps - refere-se a descrição da forma dos dados\n",
    "\n",
    "* Input_dim - refere-se ao número de entradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estrutura da Rede Neural Recorrente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Sequential()\n",
    "regressor.add(LSTM(units = 100, return_sequences = True, input_shape = (previsores.shape[1],6)))\n",
    "regressor.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(Dense(units = 1,activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.compile(optimizer = 'rmsprop', loss = 'mean_squared_error',\n",
    "                  metrics = ['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para termos acesso às estatísticas e detalhes matemáticos do modelo, usaremos uma classe do Keras, chamada callbacks.\n",
    "\n",
    "* A função **EarlyStopping** para o treinamento, quando alguma métrica não apresenta bons resultados durante o treino\n",
    "\n",
    "* A função **ReduceLROnPlateau** reduz a taxa de aprendizagem, quando alguma métrica não está funcionando bem\n",
    "\n",
    "* A função **ModelCheckpoint** salva os pesos após cada epóca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'loss', min_delta = 1e-10, patience = 10, verbose = 1)\n",
    "\n",
    "rlr = ReduceLROnPlateau(monitor = 'loss', factor = 0.2, patience = 5, verbose = 1)\n",
    "\n",
    "mcp = ModelCheckpoint(filepath = 'pesos.h5', monitor = 'loss', save_best_only = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.02130, saving model to pesos.h5\n",
      "Epoch 2/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0076 - mean_squared_error: 0.0076\n",
      "\n",
      "Epoch 00002: loss improved from 0.02130 to 0.00759, saving model to pesos.h5\n",
      "Epoch 3/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0063 - mean_squared_error: 0.0063\n",
      "\n",
      "Epoch 00003: loss improved from 0.00759 to 0.00628, saving model to pesos.h5\n",
      "Epoch 4/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "\n",
      "Epoch 00004: loss improved from 0.00628 to 0.00507, saving model to pesos.h5\n",
      "Epoch 5/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0045 - mean_squared_error: 0.0045\n",
      "\n",
      "Epoch 00005: loss improved from 0.00507 to 0.00450, saving model to pesos.h5\n",
      "Epoch 6/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0038 - mean_squared_error: 0.0038\n",
      "\n",
      "Epoch 00006: loss improved from 0.00450 to 0.00380, saving model to pesos.h5\n",
      "Epoch 7/100\n",
      "1152/1152 [==============================] - 6s 6ms/step - loss: 0.0040 - mean_squared_error: 0.0040\n",
      "\n",
      "Epoch 00007: loss did not improve from 0.00380\n",
      "Epoch 8/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0032 - mean_squared_error: 0.0032\n",
      "\n",
      "Epoch 00008: loss improved from 0.00380 to 0.00324, saving model to pesos.h5\n",
      "Epoch 9/100\n",
      "1152/1152 [==============================] - 11s 9ms/step - loss: 0.0035 - mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.00324\n",
      "Epoch 10/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0030 - mean_squared_error: 0.0030\n",
      "\n",
      "Epoch 00010: loss improved from 0.00324 to 0.00300, saving model to pesos.h5\n",
      "Epoch 11/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0029 - mean_squared_error: 0.0029\n",
      "\n",
      "Epoch 00011: loss improved from 0.00300 to 0.00286, saving model to pesos.h5\n",
      "Epoch 12/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0027 - mean_squared_error: 0.0027\n",
      "\n",
      "Epoch 00012: loss improved from 0.00286 to 0.00267, saving model to pesos.h5\n",
      "Epoch 13/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0024 - mean_squared_error: 0.0024\n",
      "\n",
      "Epoch 00013: loss improved from 0.00267 to 0.00240, saving model to pesos.h5\n",
      "Epoch 14/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0026 - mean_squared_error: 0.0026\n",
      "\n",
      "Epoch 00014: loss did not improve from 0.00240\n",
      "Epoch 15/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0026 - mean_squared_error: 0.0026\n",
      "\n",
      "Epoch 00015: loss did not improve from 0.00240\n",
      "Epoch 16/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0023 - mean_squared_error: 0.0023\n",
      "\n",
      "Epoch 00016: loss improved from 0.00240 to 0.00226, saving model to pesos.h5\n",
      "Epoch 17/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0023 - mean_squared_error: 0.0023\n",
      "\n",
      "Epoch 00017: loss improved from 0.00226 to 0.00226, saving model to pesos.h5\n",
      "Epoch 18/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0021 - mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00018: loss improved from 0.00226 to 0.00206, saving model to pesos.h5\n",
      "Epoch 19/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0020 - mean_squared_error: 0.0020\n",
      "\n",
      "Epoch 00019: loss improved from 0.00206 to 0.00202, saving model to pesos.h5\n",
      "Epoch 20/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0020 - mean_squared_error: 0.0020\n",
      "\n",
      "Epoch 00020: loss improved from 0.00202 to 0.00195, saving model to pesos.h5\n",
      "Epoch 21/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0022 - mean_squared_error: 0.0022\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.00195\n",
      "Epoch 22/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0017 - mean_squared_error: 0.0017\n",
      "\n",
      "Epoch 00022: loss improved from 0.00195 to 0.00171, saving model to pesos.h5\n",
      "Epoch 23/100\n",
      "1152/1152 [==============================] - 6s 6ms/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "\n",
      "Epoch 00023: loss improved from 0.00171 to 0.00161, saving model to pesos.h5\n",
      "Epoch 24/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0017 - mean_squared_error: 0.0017\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.00161\n",
      "Epoch 25/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0017 - mean_squared_error: 0.0017\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.00161\n",
      "Epoch 26/100\n",
      "1152/1152 [==============================] - 6s 6ms/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "\n",
      "Epoch 00026: loss improved from 0.00161 to 0.00160, saving model to pesos.h5\n",
      "Epoch 27/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0017 - mean_squared_error: 0.0017\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.00160\n",
      "Epoch 28/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0017 - mean_squared_error: 0.0017\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.00160\n",
      "Epoch 29/100\n",
      "1152/1152 [==============================] - 6s 6ms/step - loss: 0.0012 - mean_squared_error: 0.0012\n",
      "\n",
      "Epoch 00029: loss improved from 0.00160 to 0.00115, saving model to pesos.h5\n",
      "Epoch 30/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0011 - mean_squared_error: 0.0011\n",
      "\n",
      "Epoch 00030: loss improved from 0.00115 to 0.00110, saving model to pesos.h5\n",
      "Epoch 31/100\n",
      "1152/1152 [==============================] - 6s 6ms/step - loss: 0.0012 - mean_squared_error: 0.0012\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.00110\n",
      "Epoch 32/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0012 - mean_squared_error: 0.0012\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.00110\n",
      "Epoch 33/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0011 - mean_squared_error: 0.0011\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.00110\n",
      "Epoch 34/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0010 - mean_squared_error: 0.0010\n",
      "\n",
      "Epoch 00034: loss improved from 0.00110 to 0.00103, saving model to pesos.h5\n",
      "Epoch 35/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0010 - mean_squared_error: 0.0010\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.00103\n",
      "Epoch 36/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0010 - mean_squared_error: 0.0010\n",
      "\n",
      "Epoch 00036: loss improved from 0.00103 to 0.00102, saving model to pesos.h5\n",
      "Epoch 37/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0010 - mean_squared_error: 0.0010\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.00102\n",
      "Epoch 38/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0010 - mean_squared_error: 0.0010\n",
      "\n",
      "Epoch 00038: loss improved from 0.00102 to 0.00101, saving model to pesos.h5\n",
      "Epoch 39/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0010 - mean_squared_error: 0.0010    \n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 00039: loss improved from 0.00101 to 0.00100, saving model to pesos.h5\n",
      "Epoch 40/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.4396e-04 - mean_squared_error: 9.4396e-04\n",
      "\n",
      "Epoch 00040: loss improved from 0.00100 to 0.00094, saving model to pesos.h5\n",
      "Epoch 41/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0010 - mean_squared_error: 0.0010    \n",
      "\n",
      "Epoch 00041: loss did not improve from 0.00094\n",
      "Epoch 42/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0010 - mean_squared_error: 0.0010\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.00094\n",
      "Epoch 43/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.8938e-04 - mean_squared_error: 9.8938e-04\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.00094\n",
      "Epoch 44/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.7460e-04 - mean_squared_error: 9.7460e-04\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.00094\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.7070e-04 - mean_squared_error: 9.7070e-04\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.00094\n",
      "Epoch 46/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.5126e-04 - mean_squared_error: 9.5126e-04\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.00094\n",
      "Epoch 47/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0010 - mean_squared_error: 0.0010\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.00094\n",
      "Epoch 48/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.5673e-04 - mean_squared_error: 9.5673e-04\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.00094\n",
      "Epoch 49/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.4098e-04 - mean_squared_error: 9.4098e-04\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 00049: loss improved from 0.00094 to 0.00094, saving model to pesos.h5\n",
      "Epoch 50/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.3685e-04 - mean_squared_error: 9.3685e-04\n",
      "\n",
      "Epoch 00050: loss improved from 0.00094 to 0.00094, saving model to pesos.h5\n",
      "Epoch 51/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.1807e-04 - mean_squared_error: 9.1807e-04\n",
      "\n",
      "Epoch 00051: loss improved from 0.00094 to 0.00092, saving model to pesos.h5\n",
      "Epoch 52/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 0.0010 - mean_squared_error: 0.0010\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.00092\n",
      "Epoch 53/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.3748e-04 - mean_squared_error: 9.3748e-04\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.00092\n",
      "Epoch 54/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 8.5329e-04 - mean_squared_error: 8.5329e-04\n",
      "\n",
      "Epoch 00054: loss improved from 0.00092 to 0.00085, saving model to pesos.h5\n",
      "Epoch 55/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.0543e-04 - mean_squared_error: 9.0543e-04\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.00085\n",
      "Epoch 56/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.1984e-04 - mean_squared_error: 9.1984e-04\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.00085\n",
      "Epoch 57/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 8.9168e-04 - mean_squared_error: 8.9168e-04\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.00085\n",
      "Epoch 58/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.6308e-04 - mean_squared_error: 9.6308e-04\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.00085\n",
      "Epoch 59/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.4882e-04 - mean_squared_error: 9.4882e-04\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.00085\n",
      "Epoch 60/100\n",
      "1152/1152 [==============================] - 6s 6ms/step - loss: 9.4062e-04 - mean_squared_error: 9.4061e-04\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.00085\n",
      "Epoch 61/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.0426e-04 - mean_squared_error: 9.0426e-04\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.00085\n",
      "Epoch 62/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.6906e-04 - mean_squared_error: 9.6906e-04\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.00085\n",
      "Epoch 63/100\n",
      "1152/1152 [==============================] - 6s 6ms/step - loss: 9.3023e-04 - mean_squared_error: 9.3023e-04\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.00085\n",
      "Epoch 64/100\n",
      "1152/1152 [==============================] - 6s 5ms/step - loss: 9.9776e-04 - mean_squared_error: 9.9776e-04\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.00085\n",
      "Epoch 00064: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fce91515128>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(previsores, preco_real, epochs = 100, batch_size = 32,\n",
    "             callbacks = [es, rlr, mcp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previsão de preços de ações**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_teste = pd.read_csv('petr4_teste.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 7)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos fazendo o uso apenas da feature **Open**, vamos extrai-la:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "preco_real_teste = base_teste.iloc[:,1:2].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos trabalhar com tipos de dados iguais, para evitar erros com a nossa Rede Neural Recorrente, para isso vamos concatenar as suas bases(treino e teste), para facilitar igualdade de tipos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [dataFrame, base_teste]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[            Date       Open       High        Low      Close  Adj Close  \\\n",
       " 0     2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       " 1     2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       " 2     2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       " 3     2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       " 4     2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       " 5     2013-01-09  19.639999  19.870001  19.459999  19.680000  18.077084   \n",
       " 6     2013-01-10  19.770000  20.049999  19.540001  19.830000  18.214869   \n",
       " 7     2013-01-11  19.850000  20.040001  19.700001  19.879999  18.260794   \n",
       " 8     2013-01-14  20.010000  20.240000  19.690001  19.719999  18.113827   \n",
       " 9     2013-01-15  20.010000  20.240000  19.690001  19.820000  18.205681   \n",
       " 10    2013-01-16  19.889999  19.889999  19.600000  19.840000  18.224054   \n",
       " 11    2013-01-17  19.860001  19.930000  19.600000  19.709999  18.104639   \n",
       " 12    2013-01-18  19.799999  19.889999  19.540001  19.580000  17.985229   \n",
       " 13    2013-01-21  19.570000  19.600000  19.270000  19.389999  17.810705   \n",
       " 14    2013-01-22  19.420000  19.610001  19.230000  19.580000  17.985229   \n",
       " 15    2013-01-23  19.420000  19.629999  19.230000  19.549999  17.957674   \n",
       " 16    2013-01-24  19.370001  19.750000  19.370001  19.610001  18.012787   \n",
       " 17    2013-01-28  19.730000  19.809999  19.270000  19.370001  17.792336   \n",
       " 18    2013-01-29  19.350000  19.370001  18.840000  19.110001  17.553513   \n",
       " 19    2013-01-30  18.990000  18.990000  18.170000  18.200001  16.717628   \n",
       " 20    2013-01-31  18.260000  18.330000  17.900000  18.080000  16.607405   \n",
       " 21    2013-02-01  18.139999  18.650000  18.120001  18.459999  16.956451   \n",
       " 22    2013-02-04  18.160000  18.350000  17.889999  18.000000  16.533920   \n",
       " 23    2013-02-05  17.280001  18.290001  17.260000  18.080000  16.607405   \n",
       " 24    2013-02-06  17.930000  18.000000  17.530001  17.600000  16.166500   \n",
       " 25    2013-02-07  17.750000  17.980000  17.320000  17.500000  16.074644   \n",
       " 26    2013-02-08  17.549999  17.780001  17.379999  17.740000  16.295095   \n",
       " 27    2013-02-13  17.950001  18.000000  17.730000  17.799999  16.350208   \n",
       " 28    2013-02-14  17.790001  17.900000  17.580000  17.670000  16.230797   \n",
       " 29    2013-02-15  17.610001  17.790001  17.530001  17.629999  16.194056   \n",
       " ...          ...        ...        ...        ...        ...        ...   \n",
       " 1215  2017-11-20  16.020000  16.020000  16.020000  16.020000  15.988025   \n",
       " 1216  2017-11-21  16.150000  16.309999  15.850000  15.900000  15.868263   \n",
       " 1217  2017-11-22  16.090000  16.240000  15.930000  16.110001  16.077845   \n",
       " 1218  2017-11-23  15.980000  16.260000  15.940000  16.190001  16.157686   \n",
       " 1219  2017-11-24  16.250000  16.370001  16.040001  16.100000  16.067865   \n",
       " 1220  2017-11-27  16.010000  16.020000  15.780000  15.870000  15.838324   \n",
       " 1221  2017-11-28  15.930000  16.040001  15.810000  15.840000  15.808384   \n",
       " 1222  2017-11-29  15.870000  15.920000  15.320000  15.330000  15.299401   \n",
       " 1223  2017-11-30  15.300000  15.470000  14.990000  15.380000  15.349302   \n",
       " 1224  2017-12-01  15.340000  15.770000  15.260000  15.610000  15.578842   \n",
       " 1225  2017-12-04  15.650000  15.800000  15.460000  15.480000  15.449101   \n",
       " 1226  2017-12-05  15.500000  15.830000  15.210000  15.310000  15.279442   \n",
       " 1227  2017-12-06  15.220000  15.700000  15.140000  15.520000  15.489023   \n",
       " 1228  2017-12-07  15.300000  15.490000  15.070000  15.260000  15.229542   \n",
       " 1229  2017-12-08  15.510000  15.680000  15.350000  15.350000  15.319362   \n",
       " 1230  2017-12-11  15.480000  15.570000  15.370000  15.380000  15.349302   \n",
       " 1231  2017-12-12  15.360000  15.490000  15.180000  15.490000  15.459082   \n",
       " 1232  2017-12-13  15.650000  15.680000  15.110000  15.180000  15.149701   \n",
       " 1233  2017-12-14  15.100000  15.310000  15.000000  15.010000  14.980041   \n",
       " 1234  2017-12-15  15.050000  15.240000  14.950000  14.950000  14.920160   \n",
       " 1235  2017-12-18  15.160000  15.330000  15.130000  15.220000  15.189621   \n",
       " 1236  2017-12-19  15.180000  15.250000  15.060000  15.140000  15.109781   \n",
       " 1237  2017-12-20  15.210000  15.300000  15.170000  15.240000  15.209581   \n",
       " 1238  2017-12-21  15.310000  15.870000  15.300000  15.860000  15.828343   \n",
       " 1239  2017-12-22  15.750000  15.890000  15.690000  15.750000  15.718563   \n",
       " 1240  2017-12-25  15.750000  15.750000  15.750000  15.750000  15.718563   \n",
       " 1241  2017-12-26  15.750000  15.990000  15.690000  15.970000  15.938125   \n",
       " 1242  2017-12-27  15.990000  16.139999  15.980000  16.049999  16.017963   \n",
       " 1243  2017-12-28  16.100000  16.129999  16.000000  16.100000  16.067865   \n",
       " 1244  2017-12-29  16.100000  16.100000  16.100000  16.100000  16.067865   \n",
       " \n",
       "           Volume  \n",
       " 0     30182600.0  \n",
       " 1     30552600.0  \n",
       " 2     36141000.0  \n",
       " 3     28069600.0  \n",
       " 4     29091300.0  \n",
       " 5     24361100.0  \n",
       " 6     17526200.0  \n",
       " 7     18223600.0  \n",
       " 8     28302400.0  \n",
       " 9     29633900.0  \n",
       " 10    16787800.0  \n",
       " 11    19719600.0  \n",
       " 12    18913900.0  \n",
       " 13    18086200.0  \n",
       " 14    23535100.0  \n",
       " 15    17200800.0  \n",
       " 16    19612600.0  \n",
       " 17    20122600.0  \n",
       " 18    27097900.0  \n",
       " 19    66985800.0  \n",
       " 20    33246400.0  \n",
       " 21    28860400.0  \n",
       " 22    32294300.0  \n",
       " 23    77332900.0  \n",
       " 24    30886100.0  \n",
       " 25    37066400.0  \n",
       " 26    31068100.0  \n",
       " 27    22328200.0  \n",
       " 28    25902900.0  \n",
       " 29    25001600.0  \n",
       " ...          ...  \n",
       " 1215         0.0  \n",
       " 1216  45817800.0  \n",
       " 1217  37444900.0  \n",
       " 1218  15403600.0  \n",
       " 1219  18790700.0  \n",
       " 1220  28445800.0  \n",
       " 1221  30429600.0  \n",
       " 1222  45973000.0  \n",
       " 1223  52811400.0  \n",
       " 1224  42703800.0  \n",
       " 1225  43821500.0  \n",
       " 1226  30228000.0  \n",
       " 1227  39238500.0  \n",
       " 1228  37281400.0  \n",
       " 1229  39584500.0  \n",
       " 1230  21281600.0  \n",
       " 1231  36201200.0  \n",
       " 1232  46828900.0  \n",
       " 1233  37177300.0  \n",
       " 1234  55668300.0  \n",
       " 1235  42760400.0  \n",
       " 1236  22639700.0  \n",
       " 1237  20149700.0  \n",
       " 1238  47219400.0  \n",
       " 1239  18708500.0  \n",
       " 1240         0.0  \n",
       " 1241  22173100.0  \n",
       " 1242  23552200.0  \n",
       " 1243  19011500.0  \n",
       " 1244         0.0  \n",
       " \n",
       " [1242 rows x 7 columns],\n",
       "           Date       Open       High        Low      Close  Adj Close  \\\n",
       " 0   2018-01-02  16.190001  16.549999  16.190001  16.549999  16.516966   \n",
       " 1   2018-01-03  16.490000  16.719999  16.370001  16.700001  16.666668   \n",
       " 2   2018-01-04  16.780001  16.959999  16.620001  16.730000  16.696608   \n",
       " 3   2018-01-05  16.700001  16.860001  16.570000  16.830000  16.796408   \n",
       " 4   2018-01-08  16.740000  17.030001  16.709999  17.030001  16.996010   \n",
       " 5   2018-01-09  17.030001  17.160000  16.959999  17.030001  16.996010   \n",
       " 6   2018-01-10  16.920000  17.049999  16.770000  16.799999  16.766466   \n",
       " 7   2018-01-11  16.879999  17.299999  16.840000  17.250000  17.215569   \n",
       " 8   2018-01-12  17.040001  17.410000  17.020000  17.299999  17.265469   \n",
       " 9   2018-01-15  17.320000  17.440001  17.150000  17.350000  17.315371   \n",
       " 10  2018-01-16  17.350000  17.840000  17.299999  17.650000  17.614771   \n",
       " 11  2018-01-17  17.920000  18.360001  17.809999  18.360001  18.323355   \n",
       " 12  2018-01-18  18.350000  18.530001  17.930000  18.219999  18.183632   \n",
       " 13  2018-01-19  18.309999  18.420000  18.030001  18.260000  18.223553   \n",
       " 14  2018-01-22  18.260000  18.469999  18.090000  18.469999  18.433134   \n",
       " 15  2018-01-23  18.400000  18.459999  18.000000  18.240000  18.203592   \n",
       " 16  2018-01-24  18.420000  19.629999  18.420000  19.340000  19.301397   \n",
       " 17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       " 18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       " 19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       " 20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       " 21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       " \n",
       "       Volume  \n",
       " 0   33461800  \n",
       " 1   55940900  \n",
       " 2   37064900  \n",
       " 3   26958200  \n",
       " 4   28400000  \n",
       " 5   35070900  \n",
       " 6   28547700  \n",
       " 7   37921500  \n",
       " 8   45912100  \n",
       " 9   28945400  \n",
       " 10  58618300  \n",
       " 11  58488900  \n",
       " 12  48575800  \n",
       " 13  33470200  \n",
       " 14  33920000  \n",
       " 15  35567700  \n",
       " 16  89768200  \n",
       " 17         0  \n",
       " 18  81989500  \n",
       " 19  55726200  \n",
       " 20  46203000  \n",
       " 21  41576600  ]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_completa = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como nosso atributos são númericos, vamos deletar a coluna Date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_completa = base_completa.drop('Date', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1264, 6)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_completa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = base_completa[len(base_completa) - len(base_teste) - 90:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, colocaremos nossos dados na mesma escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 6)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entradas = normalizador.transform(entradas)\n",
    "entradas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos colocar nossos dados de teste em uma lista. Dessa forma, iremos preencher nosso vetor X_teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_teste = []\n",
    "for i in range(90,112):\n",
    "    X_teste.append(entradas[i-90:i, 0:6])\n",
    "\n",
    "X_teste = np.array(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 90, 6)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, iremos realizar nossas previções:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = regressor.predict(X_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizarmos as previsoes, iremos fazer um processo inverso à normalização. Dessa forma, iremos visualizar os preços em escalas reais ao início. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = normalizador_previsao.inverse_transform(previsoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que as previsões não ficaram distantes aos valores reais. Dessa maneira, podemos visualizar a média das previsoes e a média do preço real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.767471"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.87454563636364"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preco_real_teste.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10707432288707608\n"
     ]
    }
   ],
   "source": [
    "diferenca = previsoes.mean() - preco_real_teste.mean()\n",
    "print(diferenca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizarmos o quanto as previsoes puderam se aproximar dos valores reais, podemos fazer um gráfico com ambos os vetores. Para isso, vamos utilizar a biblioteca **matplotlib**, para plotar esses valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(preco_real_teste, color = 'red', label = 'Preço real')\n",
    "plt.plot(previsoes, color = 'blue', label = 'Previsoes do modelo')\n",
    "plt.title('Previsões dos preços das ações')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referências**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/deep-learning-com-python-az-curso-completo/learn/lecture/10897310#questions\n",
    "\n",
    "https://translate.googleusercontent.com/translate_c?depth=1&hl=pt-BR&prev=search&rurl=translate.google.com&sl=en&sp=nmt4&u=https://stackoverflow.com/questions/38714959/understanding-keras-lstms&usg=ALkJrhgwRgxszeum-5XuG2HTiE0KM75Wog\n",
    "\n",
    "https://www.google.com/search?q=LSTM&client=ubuntu&hs=DAN&channel=fs&sxsrf=ALeKk02TzfQeiFwQ3Hw3VX9kUWN9kBiJ6g:1592424774740&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiBip7I1InqAhU0D7kGHfcuDHsQ_AUoAXoECA8QAw\n",
    "\n",
    "https://keras.io/api/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
