{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uso de uma Rede Neural Recorrente, para prever investimentos na bolsa de valores**\n",
    "\n",
    "A base de dados representa investimentos nas ações da Petrobas na bolsa de valores, com o histórico do ano de 2013 ao ano de 2018 e o valor da alta da bolsa.\n",
    "\n",
    "Você pode encontrar a base de dados, em:\n",
    "\n",
    "https://br.financas.yahoo.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importações iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando da base de dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv('petr4_treinamento.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A base de treino é composta pelas seguintes features:\n",
    "\n",
    "* Date - Referente à data de abertura da ação;\n",
    "\n",
    "* Open - Referente ao valor na abertura da ação;\n",
    "\n",
    "* High - Referente ao valor máximo da ação;\n",
    "\n",
    "* Low - Referente ao valor mínimo da ação;\n",
    "\n",
    "* Close - Referente ao valor que a ação foi fechada;\n",
    "\n",
    "* Adj Close - Referente ao valor real previsto pela ação;\n",
    "\n",
    "* Volume - Total de investimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>19.639999</td>\n",
       "      <td>19.870001</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>18.077084</td>\n",
       "      <td>24361100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2013-01-10</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.540001</td>\n",
       "      <td>19.830000</td>\n",
       "      <td>18.214869</td>\n",
       "      <td>17526200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2013-01-11</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>19.879999</td>\n",
       "      <td>18.260794</td>\n",
       "      <td>18223600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2013-01-14</td>\n",
       "      <td>20.010000</td>\n",
       "      <td>20.240000</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.719999</td>\n",
       "      <td>18.113827</td>\n",
       "      <td>28302400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2013-01-15</td>\n",
       "      <td>20.010000</td>\n",
       "      <td>20.240000</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.820000</td>\n",
       "      <td>18.205681</td>\n",
       "      <td>29633900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2013-01-16</td>\n",
       "      <td>19.889999</td>\n",
       "      <td>19.889999</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>19.840000</td>\n",
       "      <td>18.224054</td>\n",
       "      <td>16787800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2013-01-17</td>\n",
       "      <td>19.860001</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>19.709999</td>\n",
       "      <td>18.104639</td>\n",
       "      <td>19719600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>19.799999</td>\n",
       "      <td>19.889999</td>\n",
       "      <td>19.540001</td>\n",
       "      <td>19.580000</td>\n",
       "      <td>17.985229</td>\n",
       "      <td>18913900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013-01-21</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>19.270000</td>\n",
       "      <td>19.389999</td>\n",
       "      <td>17.810705</td>\n",
       "      <td>18086200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2013-01-22</td>\n",
       "      <td>19.420000</td>\n",
       "      <td>19.610001</td>\n",
       "      <td>19.230000</td>\n",
       "      <td>19.580000</td>\n",
       "      <td>17.985229</td>\n",
       "      <td>23535100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2013-01-23</td>\n",
       "      <td>19.420000</td>\n",
       "      <td>19.629999</td>\n",
       "      <td>19.230000</td>\n",
       "      <td>19.549999</td>\n",
       "      <td>17.957674</td>\n",
       "      <td>17200800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2013-01-24</td>\n",
       "      <td>19.370001</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>19.370001</td>\n",
       "      <td>19.610001</td>\n",
       "      <td>18.012787</td>\n",
       "      <td>19612600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2013-01-28</td>\n",
       "      <td>19.730000</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>19.270000</td>\n",
       "      <td>19.370001</td>\n",
       "      <td>17.792336</td>\n",
       "      <td>20122600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2013-01-29</td>\n",
       "      <td>19.350000</td>\n",
       "      <td>19.370001</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>19.110001</td>\n",
       "      <td>17.553513</td>\n",
       "      <td>27097900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2013-01-30</td>\n",
       "      <td>18.990000</td>\n",
       "      <td>18.990000</td>\n",
       "      <td>18.170000</td>\n",
       "      <td>18.200001</td>\n",
       "      <td>16.717628</td>\n",
       "      <td>66985800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2013-01-31</td>\n",
       "      <td>18.260000</td>\n",
       "      <td>18.330000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>18.080000</td>\n",
       "      <td>16.607405</td>\n",
       "      <td>33246400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>18.139999</td>\n",
       "      <td>18.650000</td>\n",
       "      <td>18.120001</td>\n",
       "      <td>18.459999</td>\n",
       "      <td>16.956451</td>\n",
       "      <td>28860400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2013-02-04</td>\n",
       "      <td>18.160000</td>\n",
       "      <td>18.350000</td>\n",
       "      <td>17.889999</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>16.533920</td>\n",
       "      <td>32294300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>17.280001</td>\n",
       "      <td>18.290001</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>18.080000</td>\n",
       "      <td>16.607405</td>\n",
       "      <td>77332900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2013-02-06</td>\n",
       "      <td>17.930000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.530001</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>16.166500</td>\n",
       "      <td>30886100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2013-02-07</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>17.980000</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>16.074644</td>\n",
       "      <td>37066400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2013-02-08</td>\n",
       "      <td>17.549999</td>\n",
       "      <td>17.780001</td>\n",
       "      <td>17.379999</td>\n",
       "      <td>17.740000</td>\n",
       "      <td>16.295095</td>\n",
       "      <td>31068100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2013-02-13</td>\n",
       "      <td>17.950001</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.730000</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>16.350208</td>\n",
       "      <td>22328200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2013-02-14</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>17.580000</td>\n",
       "      <td>17.670000</td>\n",
       "      <td>16.230797</td>\n",
       "      <td>25902900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2013-02-15</td>\n",
       "      <td>17.610001</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>17.530001</td>\n",
       "      <td>17.629999</td>\n",
       "      <td>16.194056</td>\n",
       "      <td>25001600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>2017-11-20</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>15.988025</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>16.150000</td>\n",
       "      <td>16.309999</td>\n",
       "      <td>15.850000</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>15.868263</td>\n",
       "      <td>45817800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>2017-11-22</td>\n",
       "      <td>16.090000</td>\n",
       "      <td>16.240000</td>\n",
       "      <td>15.930000</td>\n",
       "      <td>16.110001</td>\n",
       "      <td>16.077845</td>\n",
       "      <td>37444900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>16.260000</td>\n",
       "      <td>15.940000</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.157686</td>\n",
       "      <td>15403600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>2017-11-24</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>16.370001</td>\n",
       "      <td>16.040001</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>18790700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>16.010000</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>15.870000</td>\n",
       "      <td>15.838324</td>\n",
       "      <td>28445800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>15.930000</td>\n",
       "      <td>16.040001</td>\n",
       "      <td>15.810000</td>\n",
       "      <td>15.840000</td>\n",
       "      <td>15.808384</td>\n",
       "      <td>30429600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>15.870000</td>\n",
       "      <td>15.920000</td>\n",
       "      <td>15.320000</td>\n",
       "      <td>15.330000</td>\n",
       "      <td>15.299401</td>\n",
       "      <td>45973000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>15.470000</td>\n",
       "      <td>14.990000</td>\n",
       "      <td>15.380000</td>\n",
       "      <td>15.349302</td>\n",
       "      <td>52811400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>15.340000</td>\n",
       "      <td>15.770000</td>\n",
       "      <td>15.260000</td>\n",
       "      <td>15.610000</td>\n",
       "      <td>15.578842</td>\n",
       "      <td>42703800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>15.650000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>15.460000</td>\n",
       "      <td>15.480000</td>\n",
       "      <td>15.449101</td>\n",
       "      <td>43821500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>15.830000</td>\n",
       "      <td>15.210000</td>\n",
       "      <td>15.310000</td>\n",
       "      <td>15.279442</td>\n",
       "      <td>30228000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>2017-12-06</td>\n",
       "      <td>15.220000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>15.520000</td>\n",
       "      <td>15.489023</td>\n",
       "      <td>39238500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>2017-12-07</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>15.490000</td>\n",
       "      <td>15.070000</td>\n",
       "      <td>15.260000</td>\n",
       "      <td>15.229542</td>\n",
       "      <td>37281400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>15.510000</td>\n",
       "      <td>15.680000</td>\n",
       "      <td>15.350000</td>\n",
       "      <td>15.350000</td>\n",
       "      <td>15.319362</td>\n",
       "      <td>39584500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>15.480000</td>\n",
       "      <td>15.570000</td>\n",
       "      <td>15.370000</td>\n",
       "      <td>15.380000</td>\n",
       "      <td>15.349302</td>\n",
       "      <td>21281600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>2017-12-12</td>\n",
       "      <td>15.360000</td>\n",
       "      <td>15.490000</td>\n",
       "      <td>15.180000</td>\n",
       "      <td>15.490000</td>\n",
       "      <td>15.459082</td>\n",
       "      <td>36201200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>15.650000</td>\n",
       "      <td>15.680000</td>\n",
       "      <td>15.110000</td>\n",
       "      <td>15.180000</td>\n",
       "      <td>15.149701</td>\n",
       "      <td>46828900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>2017-12-14</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>15.310000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.010000</td>\n",
       "      <td>14.980041</td>\n",
       "      <td>37177300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>15.050000</td>\n",
       "      <td>15.240000</td>\n",
       "      <td>14.950000</td>\n",
       "      <td>14.950000</td>\n",
       "      <td>14.920160</td>\n",
       "      <td>55668300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>2017-12-18</td>\n",
       "      <td>15.160000</td>\n",
       "      <td>15.330000</td>\n",
       "      <td>15.130000</td>\n",
       "      <td>15.220000</td>\n",
       "      <td>15.189621</td>\n",
       "      <td>42760400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>2017-12-19</td>\n",
       "      <td>15.180000</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>15.060000</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>15.109781</td>\n",
       "      <td>22639700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>2017-12-20</td>\n",
       "      <td>15.210000</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>15.170000</td>\n",
       "      <td>15.240000</td>\n",
       "      <td>15.209581</td>\n",
       "      <td>20149700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>2017-12-21</td>\n",
       "      <td>15.310000</td>\n",
       "      <td>15.870000</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>15.860000</td>\n",
       "      <td>15.828343</td>\n",
       "      <td>47219400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>2017-12-22</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.890000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.718563</td>\n",
       "      <td>18708500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.718563</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>15.970000</td>\n",
       "      <td>15.938125</td>\n",
       "      <td>22173100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>16.139999</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>16.049999</td>\n",
       "      <td>16.017963</td>\n",
       "      <td>23552200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.129999</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>19011500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1245 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "0     2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1     2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2     2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3     2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4     2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "5     2013-01-09  19.639999  19.870001  19.459999  19.680000  18.077084   \n",
       "6     2013-01-10  19.770000  20.049999  19.540001  19.830000  18.214869   \n",
       "7     2013-01-11  19.850000  20.040001  19.700001  19.879999  18.260794   \n",
       "8     2013-01-14  20.010000  20.240000  19.690001  19.719999  18.113827   \n",
       "9     2013-01-15  20.010000  20.240000  19.690001  19.820000  18.205681   \n",
       "10    2013-01-16  19.889999  19.889999  19.600000  19.840000  18.224054   \n",
       "11    2013-01-17  19.860001  19.930000  19.600000  19.709999  18.104639   \n",
       "12    2013-01-18  19.799999  19.889999  19.540001  19.580000  17.985229   \n",
       "13    2013-01-21  19.570000  19.600000  19.270000  19.389999  17.810705   \n",
       "14    2013-01-22  19.420000  19.610001  19.230000  19.580000  17.985229   \n",
       "15    2013-01-23  19.420000  19.629999  19.230000  19.549999  17.957674   \n",
       "16    2013-01-24  19.370001  19.750000  19.370001  19.610001  18.012787   \n",
       "17    2013-01-28  19.730000  19.809999  19.270000  19.370001  17.792336   \n",
       "18    2013-01-29  19.350000  19.370001  18.840000  19.110001  17.553513   \n",
       "19    2013-01-30  18.990000  18.990000  18.170000  18.200001  16.717628   \n",
       "20    2013-01-31  18.260000  18.330000  17.900000  18.080000  16.607405   \n",
       "21    2013-02-01  18.139999  18.650000  18.120001  18.459999  16.956451   \n",
       "22    2013-02-04  18.160000  18.350000  17.889999  18.000000  16.533920   \n",
       "23    2013-02-05  17.280001  18.290001  17.260000  18.080000  16.607405   \n",
       "24    2013-02-06  17.930000  18.000000  17.530001  17.600000  16.166500   \n",
       "25    2013-02-07  17.750000  17.980000  17.320000  17.500000  16.074644   \n",
       "26    2013-02-08  17.549999  17.780001  17.379999  17.740000  16.295095   \n",
       "27    2013-02-13  17.950001  18.000000  17.730000  17.799999  16.350208   \n",
       "28    2013-02-14  17.790001  17.900000  17.580000  17.670000  16.230797   \n",
       "29    2013-02-15  17.610001  17.790001  17.530001  17.629999  16.194056   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "1215  2017-11-20  16.020000  16.020000  16.020000  16.020000  15.988025   \n",
       "1216  2017-11-21  16.150000  16.309999  15.850000  15.900000  15.868263   \n",
       "1217  2017-11-22  16.090000  16.240000  15.930000  16.110001  16.077845   \n",
       "1218  2017-11-23  15.980000  16.260000  15.940000  16.190001  16.157686   \n",
       "1219  2017-11-24  16.250000  16.370001  16.040001  16.100000  16.067865   \n",
       "1220  2017-11-27  16.010000  16.020000  15.780000  15.870000  15.838324   \n",
       "1221  2017-11-28  15.930000  16.040001  15.810000  15.840000  15.808384   \n",
       "1222  2017-11-29  15.870000  15.920000  15.320000  15.330000  15.299401   \n",
       "1223  2017-11-30  15.300000  15.470000  14.990000  15.380000  15.349302   \n",
       "1224  2017-12-01  15.340000  15.770000  15.260000  15.610000  15.578842   \n",
       "1225  2017-12-04  15.650000  15.800000  15.460000  15.480000  15.449101   \n",
       "1226  2017-12-05  15.500000  15.830000  15.210000  15.310000  15.279442   \n",
       "1227  2017-12-06  15.220000  15.700000  15.140000  15.520000  15.489023   \n",
       "1228  2017-12-07  15.300000  15.490000  15.070000  15.260000  15.229542   \n",
       "1229  2017-12-08  15.510000  15.680000  15.350000  15.350000  15.319362   \n",
       "1230  2017-12-11  15.480000  15.570000  15.370000  15.380000  15.349302   \n",
       "1231  2017-12-12  15.360000  15.490000  15.180000  15.490000  15.459082   \n",
       "1232  2017-12-13  15.650000  15.680000  15.110000  15.180000  15.149701   \n",
       "1233  2017-12-14  15.100000  15.310000  15.000000  15.010000  14.980041   \n",
       "1234  2017-12-15  15.050000  15.240000  14.950000  14.950000  14.920160   \n",
       "1235  2017-12-18  15.160000  15.330000  15.130000  15.220000  15.189621   \n",
       "1236  2017-12-19  15.180000  15.250000  15.060000  15.140000  15.109781   \n",
       "1237  2017-12-20  15.210000  15.300000  15.170000  15.240000  15.209581   \n",
       "1238  2017-12-21  15.310000  15.870000  15.300000  15.860000  15.828343   \n",
       "1239  2017-12-22  15.750000  15.890000  15.690000  15.750000  15.718563   \n",
       "1240  2017-12-25  15.750000  15.750000  15.750000  15.750000  15.718563   \n",
       "1241  2017-12-26  15.750000  15.990000  15.690000  15.970000  15.938125   \n",
       "1242  2017-12-27  15.990000  16.139999  15.980000  16.049999  16.017963   \n",
       "1243  2017-12-28  16.100000  16.129999  16.000000  16.100000  16.067865   \n",
       "1244  2017-12-29  16.100000  16.100000  16.100000  16.100000  16.067865   \n",
       "\n",
       "          Volume  \n",
       "0     30182600.0  \n",
       "1     30552600.0  \n",
       "2     36141000.0  \n",
       "3     28069600.0  \n",
       "4     29091300.0  \n",
       "5     24361100.0  \n",
       "6     17526200.0  \n",
       "7     18223600.0  \n",
       "8     28302400.0  \n",
       "9     29633900.0  \n",
       "10    16787800.0  \n",
       "11    19719600.0  \n",
       "12    18913900.0  \n",
       "13    18086200.0  \n",
       "14    23535100.0  \n",
       "15    17200800.0  \n",
       "16    19612600.0  \n",
       "17    20122600.0  \n",
       "18    27097900.0  \n",
       "19    66985800.0  \n",
       "20    33246400.0  \n",
       "21    28860400.0  \n",
       "22    32294300.0  \n",
       "23    77332900.0  \n",
       "24    30886100.0  \n",
       "25    37066400.0  \n",
       "26    31068100.0  \n",
       "27    22328200.0  \n",
       "28    25902900.0  \n",
       "29    25001600.0  \n",
       "...          ...  \n",
       "1215         0.0  \n",
       "1216  45817800.0  \n",
       "1217  37444900.0  \n",
       "1218  15403600.0  \n",
       "1219  18790700.0  \n",
       "1220  28445800.0  \n",
       "1221  30429600.0  \n",
       "1222  45973000.0  \n",
       "1223  52811400.0  \n",
       "1224  42703800.0  \n",
       "1225  43821500.0  \n",
       "1226  30228000.0  \n",
       "1227  39238500.0  \n",
       "1228  37281400.0  \n",
       "1229  39584500.0  \n",
       "1230  21281600.0  \n",
       "1231  36201200.0  \n",
       "1232  46828900.0  \n",
       "1233  37177300.0  \n",
       "1234  55668300.0  \n",
       "1235  42760400.0  \n",
       "1236  22639700.0  \n",
       "1237  20149700.0  \n",
       "1238  47219400.0  \n",
       "1239  18708500.0  \n",
       "1240         0.0  \n",
       "1241  22173100.0  \n",
       "1242  23552200.0  \n",
       "1243  19011500.0  \n",
       "1244         0.0  \n",
       "\n",
       "[1245 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pré-processamento dos dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em relação ao valores NaN, podemos realizar três ações:\n",
    "\n",
    "* Exclução desses valores\n",
    "* Substituição por 0\n",
    "* Substituir pela média de cada coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         3\n",
       "High         3\n",
       "Low          3\n",
       "Close        3\n",
       "Adj Close    3\n",
       "Volume       3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visto que a quantidade de valores do tipo NaN é relativamente baixa em relação aos 1245 dados do data set, podemos optar por apagar esses registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = dataFrame.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1242, 7)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, podemos verificar se a função dropna(), conseguiu apagar estes valores nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse cenário, faremos a previsão dos valores das ações com a utilização de múltiplos previsores. Com isso, usaremos todos os atributos da base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treinamento = dataFrame.iloc[:,1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_valor_maximo = dataFrame.iloc[:,2:3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1242, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_treinamento.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso modelo trabalha com valores reais, podemos dizer assim. Dessa forma, podemos encontrar valores muito altos, que podem influenciar no treinamento. Para resolver essa problemática, podemos escolher duas opções:\n",
    "\n",
    "* Normalização dos dados\n",
    "\n",
    "* Padronização dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambas as técnicas trabalham com o um único objetivo: deixar os dados na mesma grandeza. Para isso, veremos como cada uma trabalha, de forma individual:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalização**\n",
    "\n",
    "![Min-Max fórmula](https://miro.medium.com/max/202/1*9N7QdpE_CfvkTyirk7_oWw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padronização**\n",
    "\n",
    "<img src=\"https://d1whtlypfis84e.cloudfront.net/guides/wp-content/uploads/2020/04/04155631/1426878678.png \" alt=\"drawing\" width=\"150\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste modelo, vamos utilizar a técnica de **Normalização**, visto a tratar os outliers nos nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para normalizar esse dados, vamos utilizar a função MinMaxScaler(), que irá transformar os dados em uma escala de 0 e 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizador = MinMaxScaler(feature_range = [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treinamento_normalizer = normalizador.fit_transform(base_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_valor_maximo_normalizer = normalizador.fit_transform(base_valor_maximo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938],\n",
       "       [0.7562984 ],\n",
       "       [0.78149225],\n",
       "       ...,\n",
       "       [0.57122093],\n",
       "       [0.57655039],\n",
       "       [0.57655039]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_treinamento_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77266112],\n",
       "       [0.78187106],\n",
       "       [0.79253519],\n",
       "       ...,\n",
       "       [0.57537562],\n",
       "       [0.57489089],\n",
       "       [0.57343674]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_valor_maximo_normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estrutura da base para previsão temporal I**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando trabalha-se com dados temporais é preciso definir um intervalo de tempo entre os dados e o alvo. Neste caso, podemos pegar um registro que tenha registros anteriores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo: \n",
    "\n",
    "| Dia da semana | Dia | Preço |\n",
    "|---------------|-----|-------|\n",
    "| Quinta-feira  | 03  | 19,99 |\n",
    "| Sexta-feira   | 04  | 19,80 |\n",
    "| Segunda-feira | 07  | 20,33 |\n",
    "| Terça-feira   | 08  | 20,48 |\n",
    "| Quarta-feira  | 09  | 20,11 |\n",
    "\n",
    "Para prever o preço da Quarta-feira, dia 09, precisamos pegar os valores das 4 datas anteriores:\n",
    "\n",
    "| **Previsores** |   **Preço real**|\n",
    "|---------------|-----|\n",
    "| 19,99 19,80 20,33 20,48 20,11  | 20,11  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estrutura da base para previsão temporal II**\n",
    "\n",
    "* Prever o preço real através do 90 valores anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = []\n",
    "preco_abertura = []\n",
    "preco_alta = []\n",
    "\n",
    "for i in range(90, 1242):\n",
    "    previsores.append(base_treinamento_normalizer[i-90:i, 0])\n",
    "    preco_abertura.append(base_treinamento_normalizer[i, 0])\n",
    "    preco_alta.append(base_valor_maximo_normalizer[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores, preco_abertura, preco_alta = np.array(previsores), np.array(preco_abertura),np.array(preco_alta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1152, 90), (1152,), (1152,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsores.shape, preco_abertura.shape, preco_alta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = np.reshape(previsores, (previsores.shape[0], previsores.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como temos dois vetores previsores de saída, precisamos juntar os dois em apenas um vetor, para que estar no formato dos parâmetros da nossa rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preco_real = np.column_stack((preco_abertura, preco_alta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76114341, 0.76490543],\n",
       "       [0.76114341, 0.7746001 ],\n",
       "       [0.77470935, 0.78090155],\n",
       "       ...,\n",
       "       [0.57122093, 0.57537562],\n",
       "       [0.57655039, 0.57489089],\n",
       "       [0.57655039, 0.57343674]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preco_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a utilização da biblioteca Keras, nosso vetor de entradas precisa manter uma dimensão 3D com o formato:\n",
    "\n",
    "                (batch_size, timesteps, input_dim)\n",
    "Sendo:\n",
    "\n",
    "* Batch_size - refere-se ao número de exemplos de treinamento usados em uma intereção\n",
    "\n",
    "* Timesteps - refere-se a descrição da forma dos dados\n",
    "\n",
    "* Input_dim - refere-se ao número de entradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estrutura da Rede Neural Recorrente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Sequential()\n",
    "regressor.add(LSTM(units = 100, return_sequences = True, input_shape = (previsores.shape[1],1)))\n",
    "regressor.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(Dense(units = 2,activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.compile(optimizer = 'rmsprop', loss = 'mean_squared_error',\n",
    "                  metrics = ['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para termos acesso às estatísticas e detalhes matemáticos do modelo, usaremos uma classe do Keras, chamada callbacks.\n",
    "\n",
    "* A função **EarlyStopping** para o treinamento, quando alguma métrica não apresenta bons resultados durante o treino\n",
    "\n",
    "* A função **ReduceLROnPlateau** reduz a taxa de aprendizagem, quando alguma métrica não está funcionando bem\n",
    "\n",
    "* A função **ModelCheckpoint** salva os pesos após cada epóca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'loss', min_delta = 1e-10, patience = 10, verbose = 1)\n",
    "\n",
    "rlr = ReduceLROnPlateau(monitor = 'loss', factor = 0.2, patience = 5, verbose = 1)\n",
    "\n",
    "mcp = ModelCheckpoint(filepath = 'pesos.h5', monitor = 'loss', save_best_only = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1152/1152 [==============================] - 10s 8ms/step - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.01770, saving model to pesos.h5\n",
      "Epoch 2/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "\n",
      "Epoch 00002: loss improved from 0.01770 to 0.00687, saving model to pesos.h5\n",
      "Epoch 3/100\n",
      "1152/1152 [==============================] - 9s 8ms/step - loss: 0.0066 - mean_squared_error: 0.0066\n",
      "\n",
      "Epoch 00003: loss improved from 0.00687 to 0.00665, saving model to pesos.h5\n",
      "Epoch 4/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "\n",
      "Epoch 00004: loss improved from 0.00665 to 0.00510, saving model to pesos.h5\n",
      "Epoch 5/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0045 - mean_squared_error: 0.0045\n",
      "\n",
      "Epoch 00005: loss improved from 0.00510 to 0.00455, saving model to pesos.h5\n",
      "Epoch 6/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0045 - mean_squared_error: 0.0045\n",
      "\n",
      "Epoch 00006: loss improved from 0.00455 to 0.00452, saving model to pesos.h5\n",
      "Epoch 7/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "\n",
      "Epoch 00007: loss improved from 0.00452 to 0.00392, saving model to pesos.h5\n",
      "Epoch 8/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0037 - mean_squared_error: 0.0037\n",
      "\n",
      "Epoch 00008: loss improved from 0.00392 to 0.00371, saving model to pesos.h5\n",
      "Epoch 9/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0034 - mean_squared_error: 0.0034\n",
      "\n",
      "Epoch 00009: loss improved from 0.00371 to 0.00338, saving model to pesos.h5\n",
      "Epoch 10/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0032 - mean_squared_error: 0.0032\n",
      "\n",
      "Epoch 00010: loss improved from 0.00338 to 0.00316, saving model to pesos.h5\n",
      "Epoch 11/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0030 - mean_squared_error: 0.0030\n",
      "\n",
      "Epoch 00011: loss improved from 0.00316 to 0.00302, saving model to pesos.h5\n",
      "Epoch 12/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0027 - mean_squared_error: 0.0027\n",
      "\n",
      "Epoch 00012: loss improved from 0.00302 to 0.00274, saving model to pesos.h5\n",
      "Epoch 13/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0027 - mean_squared_error: 0.0027\n",
      "\n",
      "Epoch 00013: loss improved from 0.00274 to 0.00268, saving model to pesos.h5\n",
      "Epoch 14/100\n",
      "1152/1152 [==============================] - 6s 6ms/step - loss: 0.0026 - mean_squared_error: 0.0026\n",
      "\n",
      "Epoch 00014: loss improved from 0.00268 to 0.00262, saving model to pesos.h5\n",
      "Epoch 15/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0026 - mean_squared_error: 0.0026\n",
      "\n",
      "Epoch 00015: loss improved from 0.00262 to 0.00258, saving model to pesos.h5\n",
      "Epoch 16/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0023 - mean_squared_error: 0.0023\n",
      "\n",
      "Epoch 00016: loss improved from 0.00258 to 0.00235, saving model to pesos.h5\n",
      "Epoch 17/100\n",
      "1152/1152 [==============================] - 9s 8ms/step - loss: 0.0022 - mean_squared_error: 0.0022\n",
      "\n",
      "Epoch 00017: loss improved from 0.00235 to 0.00223, saving model to pesos.h5\n",
      "Epoch 18/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0024 - mean_squared_error: 0.0024\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.00223\n",
      "Epoch 19/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0023 - mean_squared_error: 0.0023\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.00223\n",
      "Epoch 20/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0022 - mean_squared_error: 0.0022\n",
      "\n",
      "Epoch 00020: loss improved from 0.00223 to 0.00216, saving model to pesos.h5\n",
      "Epoch 21/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0023 - mean_squared_error: 0.0023\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.00216\n",
      "Epoch 22/100\n",
      "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0022 - mean_squared_error: 0.0022\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.00216\n",
      "Epoch 23/100\n",
      "1152/1152 [==============================] - 10s 8ms/step - loss: 0.0018 - mean_squared_error: 0.0018\n",
      "\n",
      "Epoch 00023: loss improved from 0.00216 to 0.00176, saving model to pesos.h5\n",
      "Epoch 24/100\n",
      "1152/1152 [==============================] - 9s 7ms/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "\n",
      "Epoch 00024: loss improved from 0.00176 to 0.00158, saving model to pesos.h5\n",
      "Epoch 25/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0017 - mean_squared_error: 0.0017\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.00158\n",
      "Epoch 26/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.00158\n",
      "Epoch 27/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.00158\n",
      "Epoch 28/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00028: loss improved from 0.00158 to 0.00155, saving model to pesos.h5\n",
      "Epoch 29/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 00029: loss improved from 0.00155 to 0.00154, saving model to pesos.h5\n",
      "Epoch 30/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.00154\n",
      "Epoch 31/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00031: loss improved from 0.00154 to 0.00151, saving model to pesos.h5\n",
      "Epoch 32/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.00151\n",
      "Epoch 33/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00033: loss improved from 0.00151 to 0.00150, saving model to pesos.h5\n",
      "Epoch 34/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.00150\n",
      "Epoch 35/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.00150\n",
      "Epoch 36/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.00150\n",
      "Epoch 37/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00037: loss improved from 0.00150 to 0.00148, saving model to pesos.h5\n",
      "Epoch 38/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.00148\n",
      "Epoch 39/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.00148\n",
      "Epoch 40/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0014 - mean_squared_error: 0.0014\n",
      "\n",
      "Epoch 00040: loss improved from 0.00148 to 0.00143, saving model to pesos.h5\n",
      "Epoch 41/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.00143\n",
      "Epoch 42/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0014 - mean_squared_error: 0.0014\n",
      "\n",
      "Epoch 00042: loss improved from 0.00143 to 0.00141, saving model to pesos.h5\n",
      "Epoch 43/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.00141\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.00141\n",
      "Epoch 45/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0014 - mean_squared_error: 0.0014\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.00141\n",
      "Epoch 46/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.00141\n",
      "Epoch 47/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.00141\n",
      "Epoch 48/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.00141\n",
      "Epoch 49/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.00141\n",
      "Epoch 50/100\n",
      "1152/1152 [==============================] - 8s 7ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "\n",
      "Epoch 00050: loss did not improve from 0.00141\n",
      "Epoch 51/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.00141\n",
      "Epoch 52/100\n",
      "1152/1152 [==============================] - 7s 6ms/step - loss: 0.0015 - mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.00141\n",
      "Epoch 00052: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe8241f8470>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(previsores, preco_real, epochs = 100, batch_size = 32,\n",
    "             callbacks = [es, rlr, mcp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previsão de preços de ações**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_teste = pd.read_csv('petr4_teste.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 7)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos fazendo o uso apenas da feature **Open**, vamos extrai-la:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preco_real_open = base_teste.iloc[:,1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preco_real_alta = base_teste.iloc[:,2:3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_completa = pd.concat((dataFrame['Open'], base_teste['Open']), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = base_completa[len(base_completa) - len(base_teste) - 90:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, colocaremos nossos dados na mesma escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = entradas.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entradas = normalizador.transform(entradas)\n",
    "entradas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos colocar nossos dados de teste em uma lista. Dessa forma, iremos preencher nosso vetor X_teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_teste = []\n",
    "for i in range(90,112):\n",
    "    X_teste.append(entradas[i-90:i, 0:6])\n",
    "\n",
    "X_teste = np.array(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 90, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, iremos realizar nossas previções:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = regressor.predict(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57007504, 0.5781905 ],\n",
       "       [0.5733316 , 0.58159286],\n",
       "       [0.5768242 , 0.58516526],\n",
       "       [0.5828314 , 0.5912288 ],\n",
       "       [0.5909964 , 0.5994748 ],\n",
       "       [0.599182  , 0.6077787 ],\n",
       "       [0.6064045 , 0.61513793],\n",
       "       [0.6115416 , 0.62039447],\n",
       "       [0.61391586, 0.6228385 ],\n",
       "       [0.6145319 , 0.62345994],\n",
       "       [0.61607   , 0.6249586 ],\n",
       "       [0.62010854, 0.62895846],\n",
       "       [0.62878203, 0.63766277],\n",
       "       [0.6432828 , 0.65233105],\n",
       "       [0.6606091 , 0.6699709 ],\n",
       "       [0.675345  , 0.68509406],\n",
       "       [0.68421763, 0.6943323 ],\n",
       "       [0.68702704, 0.69740903],\n",
       "       [0.69010866, 0.7006555 ],\n",
       "       [0.6992042 , 0.7098658 ],\n",
       "       [0.71448606, 0.7253047 ],\n",
       "       [0.73135155, 0.7424119 ]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizarmos as previsoes, iremos fazer um processo inverso à normalização. Dessa forma, iremos visualizar os preços em escalas reais ao início. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = normalizador.inverse_transform(previsoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizarmos o quanto as previsoes puderam se aproximar dos valores reais, podemos fazer um gráfico com ambos os vetores. Para isso, vamos utilizar a biblioteca **matplotlib**, para plotar esses valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(preco_abertura, color = 'red', label = 'Preço real abertura')\n",
    "plt.plot(preco_alta, color = 'pink', label = 'Preço real alta')\n",
    "\n",
    "plt.plot(previsoes[:,0], color = 'blue', label = 'Previsoes abertura')\n",
    "plt.plot(previsoes[:,1], color = 'orange', label = 'Previsoes alta')\n",
    "         \n",
    "plt.title('Previsões dos preços das ações')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referências**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.udemy.com/course/deep-learning-com-python-az-curso-completo/learn/lecture/10897310#questions\n",
    "\n",
    "https://translate.googleusercontent.com/translate_c?depth=1&hl=pt-BR&prev=search&rurl=translate.google.com&sl=en&sp=nmt4&u=https://stackoverflow.com/questions/38714959/understanding-keras-lstms&usg=ALkJrhgwRgxszeum-5XuG2HTiE0KM75Wog\n",
    "\n",
    "https://www.google.com/search?q=LSTM&client=ubuntu&hs=DAN&channel=fs&sxsrf=ALeKk02TzfQeiFwQ3Hw3VX9kUWN9kBiJ6g:1592424774740&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiBip7I1InqAhU0D7kGHfcuDHsQ_AUoAXoECA8QAw\n",
    "\n",
    "https://keras.io/api/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
